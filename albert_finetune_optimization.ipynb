{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "albert_glue_fine_tuning_tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SJfpgTccDB"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-research/albert/blob/master/albert_glue_fine_tuning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHQH4OCHZ9bq",
        "cellView": "form"
      },
      "source": [
        "# @title Copyright 2020 The ALBERT Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkTLZ3I4_7c_"
      },
      "source": [
        "# ALBERT End to End (Fine-tuning + Predicting) with Cloud TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtjs1QDb3DX"
      },
      "source": [
        "## Overview\n",
        "\n",
        "ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation.\n",
        "\n",
        "For a technical description of the algorithm, see our paper:\n",
        "\n",
        "https://arxiv.org/abs/1909.11942\n",
        "\n",
        "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut\n",
        "\n",
        "This Colab demonstates using a free Colab Cloud TPU to fine-tune GLUE tasks built on top of pretrained ALBERT models and \n",
        "run predictions on tuned model. The colab demonsrates loading pretrained ALBERT models from both [TF Hub](https://www.tensorflow.org/hub) and checkpoints.\n",
        "\n",
        "**Note:**  You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud \n",
        "Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) for how to create GCP account and GCS bucket. You have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs.\n",
        "\n",
        "This notebook is hosted on GitHub. To view it in its original repository, after opening the notebook, select **File > View on GitHub**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-JXlueIuPH"
      },
      "source": [
        "### Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkof5uHaQ_c"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n",
        "\n",
        "   1. Create a Cloud Storage bucket for your TensorBoard logs at http://console.cloud.google.com/storage and fill in the BUCKET parameter in the \"Parameters\" section below.\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Click Runtime again and select **Runtime > Run All** (Watch out: the \"Colab-only auth for this notebook and the TPU\" cell requires user input). You can also run the cells manually with Shift-ENTER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdMmwCJFaT8F"
      },
      "source": [
        "### Set up your TPU environment\n",
        "\n",
        "In this section, you perform the following tasks:\n",
        "\n",
        "*   Set up a Colab TPU running environment\n",
        "*   Verify that you are connected to a TPU device\n",
        "*   Upload your credentials to TPU to access your GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP"
      },
      "source": [
        "# TODO(lanzhzh): Add support for 2.x.\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import pprint\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "assert \"COLAB_TPU_ADDR\" in os.environ, \"ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\"\n",
        "TPU_ADDRESS = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"] \n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "print(\"TPU address is\", TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBP35oCDmbF"
      },
      "source": [
        "### Prepare and import ALBERT modules\n",
        "​\n",
        "With your environment configured, you can now prepare and import the ALBERT modules. The following step clones the source code from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "4e09f611-aef2-429a-ce1b-9c920d76d346"
      },
      "source": [
        "#TODO(lanzhzh): Add pip support\n",
        "import sys\n",
        "\n",
        "!test -d albert || git clone https://github.com/google-research/albert albert\n",
        "if not 'albert' in sys.path:\n",
        "  sys.path += ['albert']\n",
        "  \n",
        "!pip install sentencepiece\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'albert'...\n",
            "remote: Enumerating objects: 367, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 367 (delta 5), reused 6 (delta 3), pack-reused 353\u001b[K\n",
            "Receiving objects: 100% (367/367), 262.23 KiB | 3.50 MiB/s, done.\n",
            "Resolving deltas: 100% (237/237), done.\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRu1aKO1D7-Z"
      },
      "source": [
        "## Prepare for training\n",
        "\n",
        "This next section of code performs the following tasks:\n",
        "\n",
        "*  Specify GS bucket, create output directory for model checkpoints and eval results.\n",
        "*  Specify task and download training data.\n",
        "*  Specify ALBERT pretrained model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download GLUE data\n",
        "!git clone https://github.com/nyu-mll/GLUE-baselines download_glue\n",
        "\n",
        "GLUE_DIR='glue_data'\n",
        "!python download_glue/download_glue_data.py --data_dir $GLUE_DIR --tasks all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svpsMNG-vuko",
        "outputId": "efd19c83-68e5-4ce8-ac53-1ef53dc927e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'download_glue'...\n",
            "remote: Enumerating objects: 891, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 891 (delta 1), reused 2 (delta 0), pack-reused 886\u001b[K\n",
            "Receiving objects: 100% (891/891), 1.48 MiB | 9.17 MiB/s, done.\n",
            "Resolving deltas: 100% (610/610), done.\n",
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tError downloading standard development IDs for MRPC. You will need to manually split your data.\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "42d23ef4-2a85-46ad-d2b6-2dfe1db9c48f"
      },
      "source": [
        "# Please find the full list of tasks and their fintuning hyperparameters\n",
        "# here https://github.com/google-research/albert/blob/master/run_glue.sh\n",
        "\n",
        "BUCKET = \"luanps\" #@param { type: \"string\" }\n",
        "TASK = 'RTE' #@param {type:\"string\"}\n",
        "# Available pretrained model checkpoints:\n",
        "#   base, large, xlarge, xxlarge\n",
        "ALBERT_MODEL = 'base' #@param {type:\"string\"}\n",
        "\n",
        "TASK_DATA_DIR = 'glue_data'\n",
        "\n",
        "BASE_DIR = \"gs://\" + BUCKET\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BUCKET.\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "OUTPUT_DIR = 'gs://{}/albert-tfhub/models/{}'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Download glue data.\n",
        "#! test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo\n",
        "#!python download_glue_repo/download_glue_data.py --data_dir=$TASK_DATA_DIR --tasks=$TASK\n",
        "#print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
        "\n",
        "ALBERT_MODEL_HUB = 'https://tfhub.dev/google/albert_' + ALBERT_MODEL + '/3'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Model output directory: gs://luanps/albert-tfhub/models/RTE *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcpfl4N2EdOk"
      },
      "source": [
        "Now let's run the fine-tuning scripts. If you use the default MRPC task, this should be finished in around 10 mintues and you will get an accuracy of around 86.5."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose hyperparameters using [Optuna](https://optuna.readthedocs.io/en/stable/index.html)"
      ],
      "metadata": {
        "id": "4v1sCZqXK36L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Optuna optimzation lib\n",
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkVoRs_SD0KU",
        "outputId": "4e88d7e1-05b1-47ee-8638-6bf09de4f41b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.6 alembic-1.7.5 autopage-0.4.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import uuid"
      ],
      "metadata": {
        "id": "lJsGSNqcI4gV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_acc_from_file(result_file):\n",
        "    f = open(result_file,'r')\n",
        "    results = f.readlines()\n",
        "    result_dict = dict()\n",
        "    for r in results:\n",
        "        if 'eval_accuracy' in r:\n",
        "            k,v = r.split(' = ')\n",
        "    return float(v)"
      ],
      "metadata": {
        "id": "iOJRxUusF-mH"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "float(5e5)*10"
      ],
      "metadata": {
        "id": "CYeLutv2Qe6x",
        "outputId": "07856c00-2b1b-4b6f-b315-38630d1db28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000000.0"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    #hyperparameter setting: RTE task\n",
        "    warmup_steps = trial.suggest_int('warmup_steps', 5,15,5)#100, 500,100)\n",
        "    train_steps = trial.suggest_int('train_steps', 10,100,10) #400, 2000,100)\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128,16)\n",
        "\n",
        "    #Tmp config\n",
        "    id = str(uuid.uuid4()).split('-')[0]\n",
        "    OUTPUT_TMP = f'{OUTPUT_DIR}/{id}'\n",
        "    os.environ['TFHUB_CACHE_DIR'] = OUTPUT_TMP\n",
        "\n",
        "    !python -m albert.run_classifier \\\n",
        "            --data_dir=\"glue_data/\" \\\n",
        "            --output_dir=$OUTPUT_TMP \\\n",
        "            --albert_hub_module_handle=$ALBERT_MODEL_HUB \\\n",
        "            --spm_model_file=\"from_tf_hub\" \\\n",
        "            --do_train=True \\\n",
        "            --do_eval=True \\\n",
        "            --do_predict=False \\\n",
        "            --max_seq_length=512 \\\n",
        "            --optimizer=adamw \\\n",
        "            --task_name=$TASK \\\n",
        "            --warmup_step=$warmup_steps \\\n",
        "            --learning_rate=$learning_rate \\\n",
        "            --train_step=$train_steps \\\n",
        "            --save_checkpoints_steps=100 \\\n",
        "            --train_batch_size=$batch_size\\\n",
        "            --tpu_name=$TPU_ADDRESS \\\n",
        "            --use_tpu=True\n",
        "\n",
        "    #Download results and load model accuracy\n",
        "    !gsutil cp $OUTPUT_TMP/eval_results.txt \n",
        "    model_acc = get_last_acc_from_file(f'eval_results.txt')\n",
        "    return model_acc"
      ],
      "metadata": {
        "id": "1KfYFZIYEGUA"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run Optuna optimization\n",
        "study = optuna.create_study(direction='maximize',study_name=TASK)\n",
        "study.optimize(objective, n_trials=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeHjYq30Ix2X",
        "outputId": "863ca4f4-32af-4dee-a12f-a717263bbca6"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-09 00:35:02,612]\u001b[0m A new study created in memory with name: RTE\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "I0109 00:35:36.142469 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.143696 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.144931 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.146044 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.147236 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.148418 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.149438 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.150388 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.151288 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.152683 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.153638 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.154790 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.156116 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.157180 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.158087 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.159084 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.160547 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.161661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.162672 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.163624 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.164945 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.166373 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.167368 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.168234 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.169235 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.170184 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.171324 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.172584 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.173742 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.174894 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.176822 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.178340 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.179401 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.180723 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.181930 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.183191 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.184740 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.185999 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.187269 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.188374 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.189381 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.190419 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.191611 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.192780 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.194043 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.195407 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.196655 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.198012 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.199266 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.200493 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.201596 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.202669 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.203623 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.204815 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.205837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.206815 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.207831 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.208786 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.209922 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.211018 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.212047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.213021 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.214145 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.215485 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.216632 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.217649 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.218657 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.219636 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.220654 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.221628 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.222817 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.224007 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.225070 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.226504 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.227799 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.228786 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.230110 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.231741 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.233211 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.234924 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.236347 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.237590 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.238796 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.239945 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.241215 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.242323 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.243336 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.244411 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.245665 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.246757 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.247926 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.249167 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.250190 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.251348 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.252454 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.253480 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.254837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.255963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.256942 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.257878 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.258813 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.259793 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.260809 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.262058 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.263158 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.264141 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.265065 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.266237 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.267355 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.268341 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.269263 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.270179 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.271261 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.272335 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.273490 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.274623 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.275760 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.276934 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.278158 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.279802 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.281324 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.282466 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.283799 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.285006 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.286049 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.287011 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.287970 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.288897 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.289877 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.290831 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.291795 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.293039 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.294118 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.295155 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.296651 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.298305 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.299705 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.301152 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.302423 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.303663 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.305120 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.306641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.308118 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.309482 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.310567 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.311823 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.312898 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.313918 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.315091 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.316432 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.317756 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.318782 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.319871 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.320939 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.322043 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.323132 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.324193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.325171 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.326063 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.327066 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.327977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.328836 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.329769 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.330730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.331729 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.332776 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.333681 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.334592 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.335527 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.336546 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.337501 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.338419 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.339334 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.340294 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.341255 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.342180 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.343221 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.344700 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.345822 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.347086 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.348196 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.349134 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.350114 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.351073 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.352075 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.353041 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.353968 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.354973 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.356024 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.357412 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.358603 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.359610 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.360542 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.361641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.362647 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.363523 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.364410 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.365456 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.366442 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.367350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.368500 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.369560 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.370436 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.371587 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.372573 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.373457 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.374528 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.375443 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.376437 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.377432 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.379005 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.380575 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.382115 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.383584 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.385242 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.386665 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.388257 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.390084 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.391691 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.394238 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.396622 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.398394 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.400250 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.401989 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.403586 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.405689 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.407641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.409384 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.410713 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.412055 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.413193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.414170 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.415367 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.416661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.417932 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.419345 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.420483 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.421388 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.422548 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.423542 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.424369 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.425193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.426273 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.427597 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.428604 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.429531 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.430464 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.431380 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.432278 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.433208 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.434119 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.435375 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.436514 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.437611 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.438795 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.439834 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.440725 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.441560 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.442489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.443420 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.444636 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.445855 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.446797 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.447736 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.448774 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.449665 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.450519 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.451531 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.452427 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.453510 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.454424 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.455382 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.456257 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.457514 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.458553 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.459440 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.460431 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.461503 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.462784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.463835 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.464786 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.465757 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.466661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.467487 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.468469 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.469503 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.470450 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.471351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.472563 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.473766 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.474760 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.475692 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.476777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.477867 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.479083 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.480056 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.481170 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.482627 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.483823 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.484777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.485775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.486730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.487673 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.489150 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.490555 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.491537 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.492747 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.493800 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.494940 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.495894 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.497107 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.498123 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.499047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.500166 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.501139 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.502016 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.503101 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.504388 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.505352 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.506165 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.507059 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.507956 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.509025 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.509928 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.511199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.512290 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.513216 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.514230 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.515378 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.516733 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.518120 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.519589 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.520664 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.521778 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.522950 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.524163 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.525269 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.526361 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.527346 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.528534 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.529855 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.530879 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.531905 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.533026 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.534054 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.534971 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.535919 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.537058 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.538094 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.539031 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.540034 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.541177 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.542196 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.543140 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.544090 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.545075 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.546060 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.547021 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.547967 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.548915 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.549813 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.550756 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.551869 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.552902 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.553893 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.554847 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.555871 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.556840 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.557830 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.558722 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.559684 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.560627 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.561915 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.563035 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.564018 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.565182 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.566215 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.567581 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.569049 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.570509 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.572068 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.573549 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.574952 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.576597 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.578079 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.579884 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.581065 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.582107 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.583231 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.584971 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.586165 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.587150 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.588140 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.589109 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.590199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.591666 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.593089 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.594527 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.596139 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.597738 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.599507 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.602902 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.604571 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.606037 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.607465 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.608941 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.610420 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.612165 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.613730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.615265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.616918 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.618406 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.619842 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.621742 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.623550 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.624688 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.625833 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.626863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.627808 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.628886 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.629888 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.630817 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.631705 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.632870 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.633866 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.634884 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.635843 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.636967 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.638257 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.639336 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.640272 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.641134 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.642103 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.643303 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.644399 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.645301 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.646174 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.647114 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.648259 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.649658 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.651167 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.652343 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.653355 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.654273 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.655238 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.656170 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.657096 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.658036 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.659095 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.660559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.661725 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.662667 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.663785 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.664805 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.665785 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.666849 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.668522 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.669793 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.670852 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.671940 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.672856 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.674003 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.674996 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.676176 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.677214 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.678376 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.679447 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.680376 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.681270 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.682115 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.683280 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.684571 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.686059 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.687641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.688656 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.689736 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.690912 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.692952 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.694577 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.696326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.697829 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.699241 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.701450 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.703261 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.704743 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.706086 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.707470 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.709266 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.710864 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.712661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.713881 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.715214 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.716273 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.717122 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.718357 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.719408 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.720671 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.721934 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.722940 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.724027 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.725265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.726219 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.727176 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.728329 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.729340 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.730323 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.731300 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.732504 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.733861 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.734951 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.736018 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.737025 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.738065 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.738967 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.739844 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.740809 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.741944 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.743064 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.744147 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.745194 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.746104 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.746987 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.747968 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.748937 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.750098 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.751160 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.752108 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.753351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.754875 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.756296 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.757254 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.758120 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.759040 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.760030 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.761016 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.762045 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.763269 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.764571 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.765623 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.766515 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.767467 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.768612 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.769630 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.770594 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.771500 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.772448 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.773701 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.774777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.775668 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.776612 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.777671 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.778882 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.779857 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.781001 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.781971 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.782887 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.783837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.784773 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.785792 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.786984 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.788626 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.789997 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.790891 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.791791 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.792690 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.793541 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.794414 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.795832 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.797194 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.798695 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.800220 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.801269 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.802211 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.803193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.804441 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.805678 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.806900 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.807914 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.808858 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.810023 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.811069 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.811989 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.813000 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.814177 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.815276 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.816252 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.817270 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.818511 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.819582 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.820464 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.821382 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.822373 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.823378 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.824262 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.825670 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.826885 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.827842 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.828837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.829849 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.830745 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.831642 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.832784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.835109 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.836152 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.837268 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.838547 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.839986 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.841400 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.842519 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.843753 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.844984 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.845993 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.846917 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.847962 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.849014 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.849942 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.851043 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.852152 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.853170 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.854179 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.855147 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.856229 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.857272 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.858289 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.859183 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.860126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.861037 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.861907 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.862809 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.863768 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.864753 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.865678 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.867005 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.868071 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.868965 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.870105 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.871163 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.872283 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.873541 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.874689 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.875804 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.876754 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.877835 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.878997 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.880102 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.881093 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.882016 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.883154 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.884465 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.885488 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.886434 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.888016 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.889536 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.890817 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.892182 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.893524 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.894903 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.896407 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.897954 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.899498 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.901012 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.902441 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.903804 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.905833 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.907556 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.909280 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.910907 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.912559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.914100 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.916103 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.918081 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.920049 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.921467 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.922724 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.923858 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.925167 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.926577 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.927964 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.929431 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.930859 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.932088 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.933318 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.934328 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.935265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.936207 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.937203 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.938229 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.939192 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.940079 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.941047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.942048 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.943095 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.944138 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.945318 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.946300 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.947222 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.948134 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.948992 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.949859 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.950797 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.951789 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.952734 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.953668 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.954701 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.955733 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.956784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.957727 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.958849 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.960029 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.961352 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.962681 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.963745 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.964701 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.965776 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.966730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.967638 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.968512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.969425 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.970361 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.971518 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.972726 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.973755 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.974706 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.975717 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.976755 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.977960 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.979021 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.980243 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.981275 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.982377 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.983350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.984288 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.985266 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.986114 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.987126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.988251 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.989244 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.990274 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.991506 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.992501 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.993700 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.995043 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.995993 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.997029 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.998050 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:36.999094 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.000447 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.001591 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.002573 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.003705 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.004788 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.005717 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.006974 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.008277 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.009279 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.010239 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.011216 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.012212 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.013115 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.014030 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.015152 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.016238 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.017280 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.018274 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.019217 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.020157 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.021034 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.021904 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.022864 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.024109 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.025236 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.026179 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.027233 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.028198 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.029145 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.030084 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.030990 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.031939 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.032865 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.033806 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.034767 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.035815 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.036779 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.037900 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.039407 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.040568 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.041639 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.042638 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.043641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.044707 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.045765 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.046872 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.047897 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.048869 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.050024 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.050963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.052101 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.053215 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.054259 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.055178 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.056089 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.057070 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.058021 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.058977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.059873 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.060830 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.061792 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.062668 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.063677 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.064670 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.065813 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.066838 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.067772 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.068729 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.069591 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.070633 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.072412 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.074167 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.075732 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.076808 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.077885 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.078908 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.079904 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.080970 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.081926 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.082879 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.083928 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.085164 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.086202 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.087098 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.088262 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.089602 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.090633 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.091601 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.092500 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.093369 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.094724 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.096187 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.097527 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.098989 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.100344 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.101887 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.103352 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.104836 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.106586 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.108427 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.110102 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.111687 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.113326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.114799 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.116186 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.118131 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.119804 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.121716 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.123184 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.124600 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.125924 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.127799 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.129488 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.131009 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.132455 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.134017 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.135453 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.137323 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.138829 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.139931 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.140895 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.141834 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.142800 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.143800 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.144833 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.145754 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.146657 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.147860 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.148963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.149921 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.151162 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.152212 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.153109 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.154028 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.155216 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.156474 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.157557 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.158540 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.159497 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.160478 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.161425 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.162412 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.163752 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.164982 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.166034 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.167329 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.168545 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.169750 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.170936 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.172066 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.173107 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.174113 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.175133 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.176082 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.176972 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.177946 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.178863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.179863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.181045 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.182065 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.183007 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.183969 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.184916 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.185961 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.186959 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.187935 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.188898 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.189857 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.190745 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.191738 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.192690 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.193599 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.194807 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.195859 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.196773 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.197819 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.198784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.199738 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.200845 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.201874 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.202837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.203732 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.205540 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.206886 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.207912 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.208806 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.209743 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.210689 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.211667 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.212712 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.213876 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.215075 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.216078 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.216992 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.217905 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.219192 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.220509 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.221536 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.222489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.223427 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.224353 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.225350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.226325 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.227245 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.228100 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.228999 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.229908 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.230880 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.232034 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.233423 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.234585 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.235930 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.237234 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.238255 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.239195 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.240202 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.241203 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.242193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.243204 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.244734 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.245898 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.247133 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.248445 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.249532 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.250495 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.251627 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.252663 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.253570 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.254509 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.255784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.256980 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.257895 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.258742 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.259644 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.260627 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.261558 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.262521 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.263430 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.264337 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.265262 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.266152 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.267050 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.268137 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.269156 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.270050 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.270918 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.271802 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.272704 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.273674 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.274661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.275589 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.276651 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.277567 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.278491 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.279404 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.280522 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.281702 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.282658 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.283718 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.284842 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.286102 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.287126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.288179 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.289223 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.290124 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.291032 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.292047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.293326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.294298 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.295168 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.296065 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.297100 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.298077 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.299186 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.300287 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.301258 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.302118 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.303673 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.304827 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.305725 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.306881 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.308201 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.309452 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.310383 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.311380 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.312752 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.314159 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.315614 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.316845 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.318494 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.320088 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.322041 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.324161 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.326383 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.328176 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.329610 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.331018 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.332236 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.333523 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.334770 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.336199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.337721 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.339150 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.340912 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.342424 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.343728 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.345528 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.346823 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.347797 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.348839 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.349804 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.350735 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.351651 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.352664 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.353660 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.354634 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.355477 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.356346 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.357346 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.358228 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.359155 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.360088 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.361054 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.362015 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.363117 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.364508 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.365592 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.366591 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.367501 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.368433 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.369367 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.370511 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.371541 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.372429 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.373499 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.374875 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.376350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.378085 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.379675 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.381026 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.382521 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.384030 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.385026 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.386020 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.387358 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.388501 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.389474 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.390574 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.391635 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.392549 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.393650 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.394648 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.395874 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.396862 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.397742 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.398644 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.399592 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.400559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.401455 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.402323 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.403432 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.404550 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.405752 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.406806 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.407722 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.408761 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.410366 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.411732 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.412773 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.413765 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.415001 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.416578 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.417794 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.418805 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.419760 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.420722 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.421646 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.422595 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.423555 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.424538 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.425573 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.426512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.427552 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.428564 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.429530 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.430470 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.431391 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.432485 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.433480 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.434421 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.435302 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.436241 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.437118 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.438017 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.438901 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.439834 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.440822 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.441889 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.442872 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.443934 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.445112 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.446123 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.447061 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.448115 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.449074 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.449963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.450899 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.451853 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.452759 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.453733 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.454920 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.455905 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.456830 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.457770 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.458730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.459619 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.460512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.461374 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.462509 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.463502 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.464467 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.465384 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.466217 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.467430 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.468439 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.469516 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.470489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.471522 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.472910 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.474006 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.475256 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.476281 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.477250 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.478140 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.479035 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.480001 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.480863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.481713 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.482573 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.483566 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.484600 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.485511 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.486545 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.487641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.488609 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.489612 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.490575 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.491524 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.492449 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.493386 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.494300 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.495288 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.496385 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.497205 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.498071 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.498983 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.499961 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.500911 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.501842 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.502971 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.504083 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.505040 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.505943 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.506831 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.507737 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.508656 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.509594 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.510607 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.511638 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.512943 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.514374 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.515321 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.516265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.517326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.518329 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.519157 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.520164 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.521156 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.522083 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.522949 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.523856 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.524951 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.525964 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.526850 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.527702 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.528581 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.529745 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.530893 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.531805 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.532723 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.533691 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.534624 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.535652 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.536943 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.538019 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.538960 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.539861 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.540861 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.542072 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.543035 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.543905 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.544876 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.545870 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.546985 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.548237 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.549268 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.550109 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.550966 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.551828 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.552785 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.553643 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.554553 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.555489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.556357 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.557441 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.558481 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.559593 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.560604 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.561513 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.562427 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.563299 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.564212 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.565167 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.566055 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.567043 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.567967 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.568862 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.569880 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.570881 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.571988 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.572974 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.573850 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.574988 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.575973 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.576874 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.577775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.578675 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.579605 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.580506 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.581353 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.582256 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.583624 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.584757 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.585775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.586712 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.587683 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.588766 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.589651 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.590469 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.591365 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.592280 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.593210 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.594123 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.595070 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.595926 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.596807 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.597692 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.598547 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.599470 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.600604 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.601615 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.602521 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.603437 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.604368 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.605345 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.606196 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.607059 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.607986 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.608900 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.609740 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.610995 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.612961 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.614593 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.616112 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.617645 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.619361 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.620977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.622533 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.623951 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.625567 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.627039 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.628398 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.629790 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.631160 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.632476 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.633970 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.635239 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.636563 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.637957 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.641448 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.642881 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.644118 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.645517 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.646641 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.647639 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.648519 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.649431 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.650302 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.651389 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.652662 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.653706 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.654855 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.655968 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.657614 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.658995 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.660348 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.661876 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.663250 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.664183 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.665113 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.665975 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.667367 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.668636 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.669540 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.670431 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.671283 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.672334 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.673351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.674228 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.675101 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.676185 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.677121 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.678216 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.679297 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.680342 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.681258 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.682375 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.683615 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.684794 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.685769 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.686612 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.687535 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.688438 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.689287 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.690148 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.691056 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.691935 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.692765 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.693913 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.694956 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.695963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.696907 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.697838 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.698782 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.699687 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.700577 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.701639 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.702575 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.703678 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.704762 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.706005 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.707023 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.707880 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.708796 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.709730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.710833 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.711820 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.712717 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.713706 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.714656 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.716795 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.718428 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.719633 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.720952 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.722234 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.723586 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.724949 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.726268 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.727638 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.729057 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.731178 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.732886 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.734444 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.735918 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.737647 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.739217 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.740598 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.742051 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.743592 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.745075 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.746502 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.747503 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.748453 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.749383 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.750511 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.751581 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.752531 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.753469 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.754402 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.755248 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.756165 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.757363 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.758353 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.759336 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.760212 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.761278 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.762199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.763386 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.764489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.765473 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.766384 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.767210 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.768353 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.769407 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.770348 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.771303 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.772469 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.773735 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.774843 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.775807 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.776670 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.777778 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.778862 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.780022 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.781036 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.781932 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.782833 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.783734 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.784675 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.785657 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.786630 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.787598 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.788555 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.789572 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.790437 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.791279 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.792135 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.793036 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.793889 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.794866 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.795801 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.796715 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.797775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.798736 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.799775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.800709 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.801559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.802584 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.803470 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.804437 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.805419 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.806415 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.807390 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.808483 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.809536 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.810468 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.811361 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.812419 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.813350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.814225 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.815276 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.816332 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.817197 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.818202 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.819406 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.820919 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.822669 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.823763 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.825430 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.826979 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.828633 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.830154 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.832244 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.833863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.836951 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.838603 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.840058 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.841455 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.842887 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.845893 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.847493 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.849235 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.850836 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.852168 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.853501 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.856719 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.857634 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.858538 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.859611 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.860528 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.861480 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.862409 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.863257 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.864161 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.865351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.866395 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.867588 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.868585 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.869633 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.870652 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.871608 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.872508 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.873377 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.874248 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.875090 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.875936 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.876772 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.877659 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.878594 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.879491 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.880333 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.881168 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.881981 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.882848 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.883959 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.884966 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.886168 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.887238 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.888181 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.889240 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.890279 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.891261 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.892356 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.893285 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.894145 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.895077 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.896056 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.897175 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.898174 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.899050 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.899971 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.900908 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.902136 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.903437 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.904489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.905459 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.906397 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.907274 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.908345 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.909546 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.910819 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.911775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.912602 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.913737 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.914933 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.915940 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.916922 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.917898 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.918859 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.919984 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.921372 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.923843 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.925568 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.926725 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.928160 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.929570 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.930930 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.932040 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.933006 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.934154 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.935175 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.936126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.937128 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.938023 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.939264 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.940497 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.941588 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.942555 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.943486 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.944388 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.945259 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.946522 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.947723 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.948797 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.949837 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.950881 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.952053 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.953221 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.954327 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.955555 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.956622 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.957989 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.959137 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.960199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.961242 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.962142 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.963126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.964044 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.965024 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.966036 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.967035 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.967960 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.968889 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.969845 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.971041 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.972326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.973416 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.974523 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.975468 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.976351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.977354 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.978264 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.979277 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.980379 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.981455 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.982674 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.983718 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.984777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.985814 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.986779 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.987829 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.989817 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.991569 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.993135 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.994505 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.995908 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.997330 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.998353 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:37.999285 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.000193 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.001074 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.002027 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.003059 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.004248 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.005728 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.006961 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.007980 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.008952 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.009871 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.011031 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.012122 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.013104 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.014261 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.015296 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.016251 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.017320 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.018428 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.019512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.020900 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.022119 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.023276 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.024900 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.026462 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.027476 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.028451 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.029637 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.030752 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.031757 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.032713 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.033743 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.034797 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.036241 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.037339 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.038405 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.039388 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.040408 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.041321 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.042230 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.043490 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.044532 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.045424 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.046365 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.047354 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.048354 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.049324 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.050484 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.051552 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.052604 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.053600 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.054710 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.055732 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.056735 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.057847 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.058876 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.059968 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.061303 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.062433 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.063369 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.064571 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.065652 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.066559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.067493 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.068411 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.069537 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.070513 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.071661 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.072965 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.074418 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.075976 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.077299 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.078401 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.079597 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.080639 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.081744 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.082783 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.083834 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.084802 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.085732 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.086785 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.087899 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.088871 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.089777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.090728 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.091670 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.092624 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.093571 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.094534 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.095489 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.096867 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.098090 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.099092 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.100263 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.101350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.102266 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.103168 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.104092 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.105085 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.106347 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.107418 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.108356 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.109487 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.110634 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.111621 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.112612 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.113559 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.114775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.115825 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.116800 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.117761 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.118764 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.119950 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.121040 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.121958 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.122903 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.123825 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.124734 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.126058 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.127458 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.128930 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.130729 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.132162 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.133777 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.135829 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.137665 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.139342 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.140979 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.142512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.144386 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.146214 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.147801 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.149267 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.151132 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.152097 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.153379 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.154573 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.155558 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.156549 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.157784 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.158945 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.159996 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.160983 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.162293 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.163362 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.164536 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.165537 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.166448 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.167394 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.168272 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.169235 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.170547 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.171933 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.173089 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.174250 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.175323 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.176393 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.177400 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.178351 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.179626 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.180975 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.182223 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.183214 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.184143 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.185236 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.186244 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.187474 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.188631 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.189673 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.190791 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.192040 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.193132 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.194105 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.195250 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.196401 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.197567 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.198629 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.199730 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.200719 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.201637 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.202869 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.204047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.205299 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:35:38.206344 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:***** Running training *****\n",
            "I0109 00:35:38.871339 139639740966784 run_classifier.py:315] ***** Running training *****\n",
            "INFO:tensorflow:  Num examples = 2490\n",
            "I0109 00:35:38.871671 139639740966784 run_classifier.py:316]   Num examples = 2490\n",
            "INFO:tensorflow:  Batch size = 80\n",
            "I0109 00:35:38.871826 139639740966784 run_classifier.py:317]   Batch size = 80\n",
            "INFO:tensorflow:  Num steps = 30\n",
            "I0109 00:35:38.871929 139639740966784 run_classifier.py:318]   Num steps = 30\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.183.170:8470) for TPU system metadata.\n",
            "I0109 00:35:39.136378 139639740966784 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.76.183.170:8470) for TPU system metadata.\n",
            "2022-01-09 00:35:39.138326: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "I0109 00:35:39.155253 139639740966784 tpu_system_metadata.py:148] Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "I0109 00:35:39.155666 139639740966784 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "I0109 00:35:39.155809 139639740966784 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "I0109 00:35:39.155909 139639740966784 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11427571705680393491)\n",
            "I0109 00:35:39.156004 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11427571705680393491)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11027356195149920114)\n",
            "I0109 00:35:39.156427 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11027356195149920114)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9365630759094707857)\n",
            "I0109 00:35:39.156538 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9365630759094707857)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7989586621220768446)\n",
            "I0109 00:35:39.156638 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7989586621220768446)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9785832277138233337)\n",
            "I0109 00:35:39.156733 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9785832277138233337)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9613850914372366153)\n",
            "I0109 00:35:39.156828 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9613850914372366153)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7293297392476955861)\n",
            "I0109 00:35:39.156924 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7293297392476955861)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6142064186121730076)\n",
            "I0109 00:35:39.157023 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6142064186121730076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3940765502806562279)\n",
            "I0109 00:35:39.157116 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3940765502806562279)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5185886070249345043)\n",
            "I0109 00:35:39.157209 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5185886070249345043)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18302436322667190563)\n",
            "I0109 00:35:39.157319 139639740966784 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18302436322667190563)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "W0109 00:35:39.165227 139639740966784 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0109 00:35:39.166022 139639740966784 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:35:39.177087 139639740966784 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:744: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0109 00:35:39.205376 139639740966784 deprecation.py:323] From /content/albert/classifier_utils.py:744: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0109 00:35:39.205791 139639740966784 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effeec54050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:35:39.221776 139639740966784 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effeec54050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:721: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0109 00:35:39.226572 139639740966784 deprecation.py:323] From /content/albert/classifier_utils.py:721: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:35:39.321478 139639740966784 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (10, 512)\n",
            "I0109 00:35:39.321880 139639740966784 classifier_utils.py:826]   name = input_ids, shape = (10, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (10, 512)\n",
            "I0109 00:35:39.322041 139639740966784 classifier_utils.py:826]   name = input_mask, shape = (10, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (10,)\n",
            "I0109 00:35:39.322144 139639740966784 classifier_utils.py:826]   name = is_real_example, shape = (10,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (10,)\n",
            "I0109 00:35:39.322255 139639740966784 classifier_utils.py:826]   name = label_ids, shape = (10,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (10, 512)\n",
            "I0109 00:35:39.322422 139639740966784 classifier_utils.py:826]   name = segment_ids, shape = (10, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:35:39.324126 139639740966784 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:35:44.203753 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.204698 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.206135 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.206650 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.210340 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.216894 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.223833 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.227151 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.228534 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.236782 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.238158 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.249125 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.251198 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.260102 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.261958 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.272430 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.274667 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.296347 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.298290 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.304630 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.305942 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.312604 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.314210 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.324132 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.325603 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.331027 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:44.332455 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.110766 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.112214 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.120634 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.122063 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.126809 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.128268 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:35:45.132839 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:35:45.240557 139639740966784 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:794: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0109 00:35:45.393950 139639740966784 deprecation.py:506] From /content/albert/classifier_utils.py:794: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:35:45.438441 139639740966784 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:35:45.438809 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:35:45.438987 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:35:45.439135 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:35:45.439257 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:35:45.439383 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:35:45.439494 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:35:45.439608 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:35:45.439716 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:35:45.439831 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:35:45.439941 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:35:45.440054 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:35:45.440155 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:35:45.440270 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:35:45.440391 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:35:45.440507 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:35:45.440616 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:35:45.440725 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:35:45.440826 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:35:45.440941 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:35:45.441050 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:35:45.441164 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:35:45.441279 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:35:45.441397 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:35:45.441506 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:35:45.441620 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:35:45.441735 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:35:45.441849 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:35:45.441958 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:35:45.442059 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:35:45.442159 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:35:45.442260 139639740966784 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:35:45.442388 139639740966784 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:++++++ warmup starts at step 0, for 10 steps ++++++\n",
            "I0109 00:35:45.458456 139639740966784 optimization.py:51] ++++++ warmup starts at step 0, for 10 steps ++++++\n",
            "INFO:tensorflow:using adamw\n",
            "I0109 00:35:45.473382 139639740966784 optimization.py:75] using adamw\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0109 00:35:45.749711 139639740966784 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0109 00:35:51.396416 139639740966784 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:35:51.472207 139639740966784 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:35:54.841172 139639740966784 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:35:55.058661 139639740966784 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:36:02.352190 139639740966784 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:36:02.895539 139639740966784 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt.\n",
            "I0109 00:36:11.108358 139639740966784 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt.\n",
            "INFO:tensorflow:gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "I0109 00:36:19.472706 139639740966784 checkpoint_management.py:95] gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "W0109 00:36:23.743827 139639740966784 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:36:24.974085 139639740966784 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "I0109 00:36:24.974511 139639740966784 session_support.py:332] Installing graceful shutdown hook.\n",
            "2022-01-09 00:36:24.974874: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0109 00:36:24.979264 139639740966784 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0109 00:36:24.981204 139639740966784 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:36:24.985258 139639740966784 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 10 seconds\n",
            "I0109 00:36:35.649444 139639740966784 tpu_estimator.py:576] Initialized TPU in 10 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:36:35.650176 139637609916160 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:36:35.650600 139637601523456 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (30) batch(es) of data to infeed.\n",
            "I0109 00:36:36.195332 139639740966784 tpu_estimator.py:600] Enqueue next (30) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (30) batch(es) of data from outfeed.\n",
            "I0109 00:36:36.195643 139639740966784 tpu_estimator.py:604] Dequeue next (30) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:36:54.431505 139637601523456 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:loss = 3.58481, step = 30\n",
            "I0109 00:37:04.491876 139639740966784 basic_session_run_hooks.py:262] loss = 3.58481, step = 30\n",
            "INFO:tensorflow:Saving checkpoints for 30 into gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt.\n",
            "I0109 00:37:04.493691 139639740966784 basic_session_run_hooks.py:606] Saving checkpoints for 30 into gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt.\n",
            "INFO:tensorflow:gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "I0109 00:37:13.533488 139639740966784 checkpoint_management.py:95] gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:37:18.082706 139639740966784 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:37:18.082955 139639740966784 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:37:18.083148 139637609916160 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:37:18.083273 139637609916160 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:37:18.083455 139639740966784 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:37:18.083573 139639740966784 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:37:18.083669 139639740966784 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:37:18.083810 139637601523456 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:37:18.083919 139637601523456 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:37:18.084047 139639740966784 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:37:18.084164 139639740966784 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 3.58481.\n",
            "I0109 00:37:18.902220 139639740966784 estimator.py:371] Loss for final step: 3.58481.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "I0109 00:37:18.902793 139639740966784 error_handling.py:101] training_loop marked as finished\n",
            "INFO:tensorflow:Writing example 0 of 280\n",
            "I0109 00:37:19.182013 139639740966784 classifier_utils.py:671] Writing example 0 of 280\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.182547 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:37:19.182836 139639740966784 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 0\n",
            "I0109 00:37:19.182902 139639740966784 classifier_utils.py:646] guid: 0\n",
            "INFO:tensorflow:tokens: [CLS] ▁dana ▁reeve , ▁the ▁widow ▁of ▁the ▁actor ▁christopher ▁reeve , ▁has ▁died ▁of ▁lung ▁cancer ▁at ▁age ▁44 , ▁according ▁to ▁the ▁christopher ▁reeve ▁foundation . [SEP] ▁christopher ▁reeve ▁had ▁an ▁accident . [SEP]\n",
            "I0109 00:37:19.182974 139639740966784 classifier_utils.py:648] tokens: [CLS] ▁dana ▁reeve , ▁the ▁widow ▁of ▁the ▁actor ▁christopher ▁reeve , ▁has ▁died ▁of ▁lung ▁cancer ▁at ▁age ▁44 , ▁according ▁to ▁the ▁christopher ▁reeve ▁foundation . [SEP] ▁christopher ▁reeve ▁had ▁an ▁accident . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 11478 24604 15 14 5151 16 14 1574 4479 24604 15 63 440 16 9223 2637 35 348 4576 15 496 20 14 4479 24604 1304 9 3 4479 24604 41 40 3351 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.183235 139639740966784 classifier_utils.py:649] input_ids: 2 11478 24604 15 14 5151 16 14 1574 4479 24604 15 63 440 16 9223 2637 35 348 4576 15 496 20 14 4479 24604 1304 9 3 4479 24604 41 40 3351 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.183416 139639740966784 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.183580 139639740966784 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:37:19.183639 139639740966784 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.184604 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:37:19.185021 139639740966784 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 1\n",
            "I0109 00:37:19.185106 139639740966784 classifier_utils.py:646] guid: 1\n",
            "INFO:tensorflow:tokens: [CLS] ▁yet , ▁we ▁now ▁are ▁discovering ▁that ▁antibiotic s ▁are ▁losing ▁their ▁effectiveness ▁against ▁illness . ▁disease - ca using ▁bacteria ▁are ▁muta ting ▁faster ▁than ▁we ▁can ▁come ▁up ▁with ▁new ▁antibiotic s ▁to ▁fight ▁the ▁new ▁variations . [SEP] ▁bacteria ▁is ▁winning ▁the ▁war ▁against ▁antibiotic s . [SEP]\n",
            "I0109 00:37:19.185181 139639740966784 classifier_utils.py:648] tokens: [CLS] ▁yet , ▁we ▁now ▁are ▁discovering ▁that ▁antibiotic s ▁are ▁losing ▁their ▁effectiveness ▁against ▁illness . ▁disease - ca using ▁bacteria ▁are ▁muta ting ▁faster ▁than ▁we ▁can ▁come ▁up ▁with ▁new ▁antibiotic s ▁to ▁fight ▁the ▁new ▁variations . [SEP] ▁bacteria ▁is ▁winning ▁the ▁war ▁against ▁antibiotic s . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 768 15 95 130 50 15799 30 20017 18 50 2281 66 14115 149 6761 9 2515 8 793 12655 10955 50 22673 1203 4233 119 95 92 340 71 29 78 20017 18 20 1074 14 78 8194 9 3 10955 25 1414 14 176 149 20017 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.185441 139639740966784 classifier_utils.py:649] input_ids: 2 768 15 95 130 50 15799 30 20017 18 50 2281 66 14115 149 6761 9 2515 8 793 12655 10955 50 22673 1203 4233 119 95 92 340 71 29 78 20017 18 20 1074 14 78 8194 9 3 10955 25 1414 14 176 149 20017 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.185646 139639740966784 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.185799 139639740966784 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: entailment (id = 0)\n",
            "I0109 00:37:19.185859 139639740966784 classifier_utils.py:652] label: entailment (id = 0)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.186995 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:37:19.187412 139639740966784 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 2\n",
            "I0109 00:37:19.187487 139639740966784 classifier_utils.py:646] guid: 2\n",
            "INFO:tensorflow:tokens: [CLS] ▁cairo ▁is ▁now ▁home ▁to ▁some ▁15 ▁million ▁people ▁ - ▁a ▁burgeoning ▁population ▁that ▁produces ▁approximately ▁10,000 ▁tonnes ▁of ▁rubbish ▁per ▁day , ▁putting ▁an ▁enormous ▁strain ▁on ▁public ▁services . ▁in ▁the ▁past ▁10 ▁years , ▁the ▁government ▁has ▁tried ▁hard ▁to ▁encourage ▁private ▁investment ▁in ▁the ▁refuse ▁sector , ▁but ▁some ▁estimate ▁4,000 ▁tonnes ▁of ▁waste ▁is ▁left ▁behind ▁every ▁day , ▁fest ering ▁in ▁the ▁heat ▁as ▁it ▁wait s ▁for ▁someone ▁to ▁clear ▁it ▁up . ▁it ▁is ▁often ▁the ▁people ▁in ▁the ▁poor est ▁neighbourhood s ▁that ▁are ▁worst ▁affected . ▁but ▁in ▁some ▁areas ▁they ▁are ▁fighting ▁back . ▁in ▁shu bra , ▁one ▁of ▁the ▁northern ▁districts ▁of ▁the ▁city , ▁the ▁residents ▁have ▁taken ▁to ▁the ▁streets ▁armed ▁with ▁dust pan s ▁and ▁brushes ▁to ▁clean ▁up ▁public ▁areas ▁which ▁have ▁been ▁used ▁as ▁public ▁dump s . [SEP] ▁15 ▁million ▁tonnes ▁of ▁rubbish ▁are ▁produced ▁daily ▁in ▁cairo . [SEP]\n",
            "I0109 00:37:19.187594 139639740966784 classifier_utils.py:648] tokens: [CLS] ▁cairo ▁is ▁now ▁home ▁to ▁some ▁15 ▁million ▁people ▁ - ▁a ▁burgeoning ▁population ▁that ▁produces ▁approximately ▁10,000 ▁tonnes ▁of ▁rubbish ▁per ▁day , ▁putting ▁an ▁enormous ▁strain ▁on ▁public ▁services . ▁in ▁the ▁past ▁10 ▁years , ▁the ▁government ▁has ▁tried ▁hard ▁to ▁encourage ▁private ▁investment ▁in ▁the ▁refuse ▁sector , ▁but ▁some ▁estimate ▁4,000 ▁tonnes ▁of ▁waste ▁is ▁left ▁behind ▁every ▁day , ▁fest ering ▁in ▁the ▁heat ▁as ▁it ▁wait s ▁for ▁someone ▁to ▁clear ▁it ▁up . ▁it ▁is ▁often ▁the ▁people ▁in ▁the ▁poor est ▁neighbourhood s ▁that ▁are ▁worst ▁affected . ▁but ▁in ▁some ▁areas ▁they ▁are ▁fighting ▁back . ▁in ▁shu bra , ▁one ▁of ▁the ▁northern ▁districts ▁of ▁the ▁city , ▁the ▁residents ▁have ▁taken ▁to ▁the ▁streets ▁armed ▁with ▁dust pan s ▁and ▁brushes ▁to ▁clean ▁up ▁public ▁areas ▁which ▁have ▁been ▁used ▁as ▁public ▁dump s . [SEP] ▁15 ▁million ▁tonnes ▁of ▁rubbish ▁are ▁produced ▁daily ▁in ▁cairo . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 11772 25 130 213 20 109 357 507 148 13 8 21 29936 350 30 6700 1357 6693 11485 16 28898 416 208 15 3873 40 7135 8302 27 317 687 9 19 14 640 332 122 15 14 283 63 794 552 20 8333 932 3709 19 14 10198 3172 15 47 109 10243 13845 11485 16 4600 25 225 439 352 208 15 11837 7882 19 14 1737 28 32 1760 18 26 737 20 1207 32 71 9 32 25 478 14 148 19 14 1696 1430 9876 18 30 50 4126 4114 9 47 19 109 924 59 50 1849 97 9 19 5728 2559 15 53 16 14 743 3872 16 14 136 15 14 2175 57 658 20 14 3240 2799 29 4346 3206 18 17 25079 20 2745 71 317 924 56 57 74 147 28 317 11424 18 9 3 357 507 11485 16 28898 50 671 1954 19 11772 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.240386 139639740966784 classifier_utils.py:649] input_ids: 2 11772 25 130 213 20 109 357 507 148 13 8 21 29936 350 30 6700 1357 6693 11485 16 28898 416 208 15 3873 40 7135 8302 27 317 687 9 19 14 640 332 122 15 14 283 63 794 552 20 8333 932 3709 19 14 10198 3172 15 47 109 10243 13845 11485 16 4600 25 225 439 352 208 15 11837 7882 19 14 1737 28 32 1760 18 26 737 20 1207 32 71 9 32 25 478 14 148 19 14 1696 1430 9876 18 30 50 4126 4114 9 47 19 109 924 59 50 1849 97 9 19 5728 2559 15 53 16 14 743 3872 16 14 136 15 14 2175 57 658 20 14 3240 2799 29 4346 3206 18 17 25079 20 2745 71 317 924 56 57 74 147 28 317 11424 18 9 3 357 507 11485 16 28898 50 671 1954 19 11772 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.241814 139639740966784 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.242210 139639740966784 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:37:19.242378 139639740966784 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.243999 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:37:19.244463 139639740966784 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 3\n",
            "I0109 00:37:19.244577 139639740966784 classifier_utils.py:646] guid: 3\n",
            "INFO:tensorflow:tokens: [CLS] ▁the ▁a mish ▁community ▁in ▁pennsylvania , ▁which ▁numbers ▁about ▁5 5,000 , ▁lives ▁an ▁agrarian ▁lifestyle , ▁shun ning ▁technological ▁advances ▁like ▁electricity ▁and ▁automobile s . ▁and ▁many ▁say ▁their ▁in s ular ▁lifestyle ▁gives ▁them ▁a ▁sense ▁that ▁they ▁are ▁protected ▁from ▁the ▁violence ▁of ▁american ▁society . ▁but ▁as ▁residents ▁gathered ▁near ▁the ▁school , ▁some ▁wearing ▁traditional ▁garb ▁and ▁arriving ▁in ▁horse - drawn ▁bug gies , ▁they ▁said ▁that ▁sense ▁of ▁safety ▁had ▁been ▁shattered . ▁ \" if ▁someone ▁snap s ▁and ▁wants ▁to ▁do ▁something ▁stupid , ▁there ' s ▁no ▁distance ▁that ' s ▁going ▁to ▁stop ▁them , \" ▁said ▁jake ▁king , ▁56 , ▁an ▁a mish ▁lantern ▁maker ▁who ▁knew ▁several ▁families ▁whose ▁children ▁had ▁been ▁shot . [SEP] ▁pennsylvania ▁has ▁the ▁biggest ▁a mish ▁community ▁in ▁the ▁u . s . [SEP]\n",
            "I0109 00:37:19.244727 139639740966784 classifier_utils.py:648] tokens: [CLS] ▁the ▁a mish ▁community ▁in ▁pennsylvania , ▁which ▁numbers ▁about ▁5 5,000 , ▁lives ▁an ▁agrarian ▁lifestyle , ▁shun ning ▁technological ▁advances ▁like ▁electricity ▁and ▁automobile s . ▁and ▁many ▁say ▁their ▁in s ular ▁lifestyle ▁gives ▁them ▁a ▁sense ▁that ▁they ▁are ▁protected ▁from ▁the ▁violence ▁of ▁american ▁society . ▁but ▁as ▁residents ▁gathered ▁near ▁the ▁school , ▁some ▁wearing ▁traditional ▁garb ▁and ▁arriving ▁in ▁horse - drawn ▁bug gies , ▁they ▁said ▁that ▁sense ▁of ▁safety ▁had ▁been ▁shattered . ▁ \" if ▁someone ▁snap s ▁and ▁wants ▁to ▁do ▁something ▁stupid , ▁there ' s ▁no ▁distance ▁that ' s ▁going ▁to ▁stop ▁them , \" ▁said ▁jake ▁king , ▁56 , ▁an ▁a mish ▁lantern ▁maker ▁who ▁knew ▁several ▁families ▁whose ▁children ▁had ▁been ▁shot . [SEP] ▁pennsylvania ▁has ▁the ▁biggest ▁a mish ▁community ▁in ▁the ▁u . s . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 14 21 10758 514 19 1806 15 56 2116 88 331 5157 15 1551 40 24463 8937 15 15800 2981 10381 10592 101 5592 17 8199 18 9 17 151 395 66 19 18 7451 8937 2352 105 21 1259 30 59 50 3803 37 14 3300 16 189 641 9 47 28 2175 4744 424 14 116 15 109 2466 1361 19646 17 6426 19 1729 8 19950 6256 19008 15 59 87 30 1259 16 2108 41 74 11915 9 13 7 821 737 6877 18 17 2846 20 107 301 3553 15 80 22 18 90 1583 30 22 18 228 20 747 105 15 7 87 3777 437 15 6171 15 40 21 10758 11585 11767 72 404 238 1250 1196 391 41 74 999 9 3 1806 63 14 3835 21 10758 514 19 14 287 9 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.244954 139639740966784 classifier_utils.py:649] input_ids: 2 14 21 10758 514 19 1806 15 56 2116 88 331 5157 15 1551 40 24463 8937 15 15800 2981 10381 10592 101 5592 17 8199 18 9 17 151 395 66 19 18 7451 8937 2352 105 21 1259 30 59 50 3803 37 14 3300 16 189 641 9 47 28 2175 4744 424 14 116 15 109 2466 1361 19646 17 6426 19 1729 8 19950 6256 19008 15 59 87 30 1259 16 2108 41 74 11915 9 13 7 821 737 6877 18 17 2846 20 107 301 3553 15 80 22 18 90 1583 30 22 18 228 20 747 105 15 7 87 3777 437 15 6171 15 40 21 10758 11585 11767 72 404 238 1250 1196 391 41 74 999 9 3 1806 63 14 3835 21 10758 514 19 14 287 9 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.245170 139639740966784 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.245384 139639740966784 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:37:19.245475 139639740966784 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.246251 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:37:19.246585 139639740966784 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 4\n",
            "I0109 00:37:19.246684 139639740966784 classifier_utils.py:646] guid: 4\n",
            "INFO:tensorflow:tokens: [CLS] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁an ▁election ▁campaign ▁in ▁which ▁more ▁than ▁1,000 ▁people , ▁including ▁seven ▁election ▁candidates , ▁have ▁been ▁killed . [SEP] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁a ▁campaign ▁marred ▁by ▁violence . [SEP]\n",
            "I0109 00:37:19.246796 139639740966784 classifier_utils.py:648] tokens: [CLS] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁an ▁election ▁campaign ▁in ▁which ▁more ▁than ▁1,000 ▁people , ▁including ▁seven ▁election ▁candidates , ▁have ▁been ▁killed . [SEP] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁a ▁campaign ▁marred ▁by ▁violence . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1221 879 46 27 183 7863 75 40 776 1150 19 56 91 119 5925 148 15 215 810 776 4074 15 57 74 841 9 3 1221 879 46 27 183 7863 75 21 1150 26479 34 3300 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.247009 139639740966784 classifier_utils.py:649] input_ids: 2 1221 879 46 27 183 7863 75 40 776 1150 19 56 91 119 5925 148 15 215 810 776 4074 15 57 74 841 9 3 1221 879 46 27 183 7863 75 21 1150 26479 34 3300 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.247216 139639740966784 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:37:19.247425 139639740966784 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: entailment (id = 0)\n",
            "I0109 00:37:19.343406 139639740966784 classifier_utils.py:652] label: entailment (id = 0)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.344914 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.346389 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.347413 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.348329 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.349214 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.350138 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.351008 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.352083 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.353172 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.354197 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.355139 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.356020 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.356991 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.357954 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.359003 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.359988 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.361103 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.362384 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.363595 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.364772 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.365731 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.366628 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.367528 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.368420 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.369673 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.370902 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.372078 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.373615 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.374932 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.375960 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.376944 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.377977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.379134 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.380051 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.380942 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.381951 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.383006 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.384417 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.385418 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.386375 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.387421 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.388394 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.389280 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.390176 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.391148 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.392127 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.393203 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.394234 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.395178 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.396081 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.396984 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.398072 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.399056 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.399906 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.400816 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.401740 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.402704 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.403618 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.404654 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.405648 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.406601 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.407486 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.408399 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.409285 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.410345 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.411321 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.412390 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.413350 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.414332 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.415356 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.416341 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.417332 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.418277 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.419386 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.420467 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.421411 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.422232 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.423096 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.423983 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.424872 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.425775 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.426674 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.427627 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.428556 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.429875 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.431049 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.431996 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.433010 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.433977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.434897 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.435993 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.437060 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.437977 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.438895 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.440054 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.441343 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.442729 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.444082 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.445644 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.447059 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.448524 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.450377 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.452013 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.453594 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.455481 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.457734 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.459421 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.460842 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.462265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.465459 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.467164 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.468645 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.470177 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.471619 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.473181 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.474670 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.475725 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.476675 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.477835 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.478830 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.480004 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.481046 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.481958 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.482840 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.483792 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.484968 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.486517 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.488006 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.489240 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.490435 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.491640 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.493007 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.494124 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.495042 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.496143 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.497068 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.497980 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.499028 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.499944 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.500795 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.501629 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.502471 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.503394 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.504289 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.505265 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.506171 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.507073 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.508011 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.509136 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.510169 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.511234 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.512259 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.513431 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.514425 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.515277 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.516246 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.517178 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.518070 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.518928 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.519846 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.520767 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.521718 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.522640 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.523514 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.524394 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.525277 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.526183 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.527129 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.528370 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.529460 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.530477 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.531580 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.532510 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.533416 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.534286 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.535225 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.536126 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.537191 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.538227 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.539132 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.539998 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.540947 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.541825 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.542810 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.543767 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.544664 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.545542 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.546475 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.547380 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.548824 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.550402 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.551378 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.552199 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.553047 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.554010 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.554987 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.555888 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.556763 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.557644 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.558545 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.559455 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.560513 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.561710 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.562654 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.563542 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.564588 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.565568 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.566461 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.567617 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.568706 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.569652 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.570590 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.571589 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.572695 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.573714 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.574570 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.575468 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.576497 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.577476 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.578507 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.579520 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.580425 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.581452 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.582462 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.583416 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.584355 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.585252 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.586144 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.587038 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.587919 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.588792 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.589878 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.590848 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.591723 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.592847 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.593869 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.594749 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.595635 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.596544 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.597836 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.598963 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.599863 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.600717 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.601616 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.602479 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.603445 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.604398 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.605247 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.606110 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.607018 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.607858 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.608888 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.609857 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.610754 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.611648 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.612512 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.613412 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.614326 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.615211 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.616136 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.617176 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.618120 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.619246 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.620288 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.621185 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.622000 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.622828 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.623666 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.624590 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.625556 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.626461 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:37:19.627516 139639740966784 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:***** Running evaluation *****\n",
            "I0109 00:37:20.123935 139639740966784 run_classifier.py:350] ***** Running evaluation *****\n",
            "INFO:tensorflow:  Num examples = 280 (277 actual, 3 padding)\n",
            "I0109 00:37:20.124219 139639740966784 run_classifier.py:353]   Num examples = 280 (277 actual, 3 padding)\n",
            "INFO:tensorflow:  Batch size = 8\n",
            "I0109 00:37:20.124404 139639740966784 run_classifier.py:354]   Batch size = 8\n",
            "INFO:tensorflow:Best trial info: Step: -1, Best Value Step: -1, Best Value: -1\n",
            "I0109 00:37:20.381660 139639740966784 run_classifier.py:391] Best trial info: Step: -1, Best Value Step: -1, Best Value: -1\n",
            "INFO:tensorflow:Add gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0 to eval list.\n",
            "I0109 00:37:20.505691 139639740966784 run_classifier.py:433] Add gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0 to eval list.\n",
            "INFO:tensorflow:Add gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30 to eval list.\n",
            "I0109 00:37:20.505907 139639740966784 run_classifier.py:433] Add gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30 to eval list.\n",
            "INFO:tensorflow:found 2 files.\n",
            "I0109 00:37:20.505973 139639740966784 run_classifier.py:435] found 2 files.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:37:20.511761 139639740966784 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effec5cc8c0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:37:20.531859 139639740966784 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effec5cc8c0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:37:20.604681 139639740966784 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (1, 512)\n",
            "I0109 00:37:20.604918 139639740966784 classifier_utils.py:826]   name = input_ids, shape = (1, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (1, 512)\n",
            "I0109 00:37:20.604996 139639740966784 classifier_utils.py:826]   name = input_mask, shape = (1, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (1,)\n",
            "I0109 00:37:20.605060 139639740966784 classifier_utils.py:826]   name = is_real_example, shape = (1,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (1,)\n",
            "I0109 00:37:20.605118 139639740966784 classifier_utils.py:826]   name = label_ids, shape = (1,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (1, 512)\n",
            "I0109 00:37:20.605176 139639740966784 classifier_utils.py:826]   name = segment_ids, shape = (1, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:37:20.606021 139639740966784 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:37:24.551709 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.552319 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.552678 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.553011 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.556503 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.562671 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.568688 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.571633 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.573524 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.582122 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.583270 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.594174 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.595845 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.604643 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.606274 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.614993 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.616749 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.634821 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.636663 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.642796 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.643977 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.650203 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.651700 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.661509 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.662639 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.667944 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:24.669069 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.749196 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.750405 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.757133 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.758203 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.762173 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.763250 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:37:25.767498 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:37:25.857870 139639740966784 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:37:26.010553 139639740966784 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:37:26.010811 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:37:26.010972 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:37:26.011105 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:37:26.011223 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:37:26.011363 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:37:26.011475 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:37:26.011590 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:37:26.011690 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:37:26.011803 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:37:26.011904 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:37:26.012008 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:37:26.012106 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:37:26.012209 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:37:26.012325 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:37:26.012438 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:37:26.012544 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:37:26.012648 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:37:26.012750 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:37:26.012859 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:37:26.012959 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:37:26.013068 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:37:26.013171 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:37:26.013273 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:37:26.013403 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:37:26.013523 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:37:26.013643 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:37:26.013767 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:37:26.013891 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:37:26.013998 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:37:26.014101 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:37:26.014209 139639740966784 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:37:26.014365 139639740966784 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0109 00:37:26.026492 139639740966784 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:37:27.124004 139639740966784 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-01-09T00:37:27Z\n",
            "I0109 00:37:27.139861 139639740966784 evaluation.py:255] Starting evaluation at 2022-01-09T00:37:27Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:37:27.140120 139639740966784 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:37:27.258301 139639740966784 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0\n",
            "I0109 00:37:27.386563 139639740966784 saver.py:1284] Restoring parameters from gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:37:32.541572 139639740966784 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:37:32.920954 139639740966784 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:37:33.644424 139639740966784 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 28 seconds\n",
            "I0109 00:38:01.968250 139639740966784 tpu_estimator.py:576] Initialized TPU in 28 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:38:01.969010 139637589726976 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:38:01.969377 139637581334272 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:38:02.326828 139639740966784 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (35) batch(es) of data to infeed.\n",
            "I0109 00:38:02.725917 139639740966784 tpu_estimator.py:600] Enqueue next (35) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0109 00:38:02.726268 139639740966784 tpu_estimator.py:604] Dequeue next (35) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:38:08.261682 139637581334272 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Evaluation [35/35]\n",
            "I0109 00:38:08.851689 139639740966784 evaluation.py:167] Evaluation [35/35]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:38:08.852038 139639740966784 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:38:08.852156 139639740966784 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:38:08.852416 139637589726976 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:38:08.852530 139637589726976 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:38:08.852655 139639740966784 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:38:08.852778 139639740966784 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:38:08.852978 139639740966784 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:38:08.853214 139637581334272 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:38:08.853331 139637581334272 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:38:08.853466 139639740966784 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:38:08.853599 139639740966784 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2022-01-09-00:38:09\n",
            "I0109 00:38:09.660912 139639740966784 evaluation.py:275] Finished evaluation at 2022-01-09-00:38:09\n",
            "INFO:tensorflow:Saving dict for global step 0: eval_accuracy = 0.41877255, eval_loss = 0.7492937, global_step = 0, loss = 0.783012\n",
            "I0109 00:38:09.661201 139639740966784 estimator.py:2049] Saving dict for global step 0: eval_accuracy = 0.41877255, eval_loss = 0.7492937, global_step = 0, loss = 0.783012\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0\n",
            "I0109 00:38:14.576256 139639740966784 estimator.py:2109] Saving 'checkpoint_path' summary for global step 0: gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-0\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "I0109 00:38:15.696301 139639740966784 error_handling.py:101] evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "I0109 00:38:15.696623 139639740966784 run_classifier.py:453] ***** Eval results *****\n",
            "INFO:tensorflow:  eval_accuracy = 0.41877255\n",
            "I0109 00:38:15.696736 139639740966784 run_classifier.py:455]   eval_accuracy = 0.41877255\n",
            "INFO:tensorflow:  eval_loss = 0.7492937\n",
            "I0109 00:38:15.697105 139639740966784 run_classifier.py:455]   eval_loss = 0.7492937\n",
            "INFO:tensorflow:  global_step = 0\n",
            "I0109 00:38:15.697434 139639740966784 run_classifier.py:455]   global_step = 0\n",
            "INFO:tensorflow:  loss = 0.783012\n",
            "I0109 00:38:15.697695 139639740966784 run_classifier.py:455]   loss = 0.783012\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:38:16.685343 139639740966784 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effec5139e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:38:16.701709 139639740966784 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7effec5139e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:38:16.787278 139639740966784 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (1, 512)\n",
            "I0109 00:38:16.787569 139639740966784 classifier_utils.py:826]   name = input_ids, shape = (1, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (1, 512)\n",
            "I0109 00:38:16.787685 139639740966784 classifier_utils.py:826]   name = input_mask, shape = (1, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (1,)\n",
            "I0109 00:38:16.787780 139639740966784 classifier_utils.py:826]   name = is_real_example, shape = (1,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (1,)\n",
            "I0109 00:38:16.787889 139639740966784 classifier_utils.py:826]   name = label_ids, shape = (1,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (1, 512)\n",
            "I0109 00:38:16.787992 139639740966784 classifier_utils.py:826]   name = segment_ids, shape = (1, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:38:16.788881 139639740966784 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:38:21.038780 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.039387 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.039742 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.040071 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.043422 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.049540 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.055284 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.058058 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.059144 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.066711 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.067775 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.082034 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.084810 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.099224 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.101412 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.110850 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.112533 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.131352 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.133124 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.138987 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.140085 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.146159 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.147647 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.157629 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.158753 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.163794 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.164900 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.958052 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.959255 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.966092 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.967207 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.971129 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.972203 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:38:21.976264 139639740966784 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:38:22.069670 139639740966784 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:38:22.220803 139639740966784 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:38:22.221066 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:38:22.221233 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:38:22.221381 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:38:22.221502 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:38:22.221611 139639740966784 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:38:22.221714 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:38:22.221830 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:38:22.221935 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:38:22.222051 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:38:22.222150 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:38:22.222254 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:38:22.222369 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:38:22.222480 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:38:22.222579 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:38:22.222682 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:38:22.222780 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:38:22.222879 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:38:22.222976 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:38:22.223081 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:38:22.223179 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:38:22.223282 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:38:22.223434 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:38:22.223541 139639740966784 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:38:22.223638 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:38:22.223742 139639740966784 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:38:22.223839 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:38:22.223954 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:38:22.224052 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:38:22.224149 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:38:22.224247 139639740966784 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:38:22.224390 139639740966784 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:38:22.224504 139639740966784 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:38:23.343605 139639740966784 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-01-09T00:38:23Z\n",
            "I0109 00:38:23.360453 139639740966784 evaluation.py:255] Starting evaluation at 2022-01-09T00:38:23Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:38:23.360733 139639740966784 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:38:23.481777 139639740966784 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30\n",
            "I0109 00:38:23.615548 139639740966784 saver.py:1284] Restoring parameters from gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:38:28.764044 139639740966784 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:38:29.117501 139639740966784 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:38:29.867830 139639740966784 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 22 seconds\n",
            "I0109 00:38:52.820251 139639740966784 tpu_estimator.py:576] Initialized TPU in 22 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:38:52.821074 139637581334272 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:38:52.821470 139637572941568 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:38:53.185848 139639740966784 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (35) batch(es) of data to infeed.\n",
            "I0109 00:38:53.507117 139639740966784 tpu_estimator.py:600] Enqueue next (35) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0109 00:38:53.507456 139639740966784 tpu_estimator.py:604] Dequeue next (35) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:38:59.323928 139637572941568 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Evaluation [35/35]\n",
            "I0109 00:38:59.913589 139639740966784 evaluation.py:167] Evaluation [35/35]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:38:59.913952 139639740966784 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:38:59.914074 139639740966784 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:38:59.914263 139637581334272 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:38:59.914393 139637581334272 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:38:59.914522 139639740966784 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:38:59.914649 139639740966784 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:38:59.914727 139639740966784 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:38:59.914866 139637572941568 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:38:59.914943 139637572941568 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:38:59.915060 139639740966784 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:38:59.915170 139639740966784 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2022-01-09-00:39:00\n",
            "I0109 00:39:00.774742 139639740966784 evaluation.py:275] Finished evaluation at 2022-01-09-00:39:00\n",
            "INFO:tensorflow:Saving dict for global step 30: eval_accuracy = 0.47292417, eval_loss = 2.2060354, global_step = 30, loss = 2.0341403\n",
            "I0109 00:39:00.775070 139639740966784 estimator.py:2049] Saving dict for global step 30: eval_accuracy = 0.47292417, eval_loss = 2.2060354, global_step = 30, loss = 2.0341403\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 30: gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30\n",
            "I0109 00:39:01.523967 139639740966784 estimator.py:2109] Saving 'checkpoint_path' summary for global step 30: gs://luanps/albert-tfhub/models/RTE/ddfccaa4/model.ckpt-30\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "I0109 00:39:02.387496 139639740966784 error_handling.py:101] evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "I0109 00:39:02.387781 139639740966784 run_classifier.py:453] ***** Eval results *****\n",
            "INFO:tensorflow:  eval_accuracy = 0.47292417\n",
            "I0109 00:39:02.387862 139639740966784 run_classifier.py:455]   eval_accuracy = 0.47292417\n",
            "INFO:tensorflow:  eval_loss = 2.2060354\n",
            "I0109 00:39:02.387962 139639740966784 run_classifier.py:455]   eval_loss = 2.2060354\n",
            "INFO:tensorflow:  global_step = 30\n",
            "I0109 00:39:02.388042 139639740966784 run_classifier.py:455]   global_step = 30\n",
            "INFO:tensorflow:  loss = 2.0341403\n",
            "I0109 00:39:02.388108 139639740966784 run_classifier.py:455]   loss = 2.0341403\n",
            "INFO:tensorflow:saving model.ckpt-30.meta to model.ckpt-best.meta\n",
            "I0109 00:39:03.395120 139639740966784 run_classifier.py:473] saving model.ckpt-30.meta to model.ckpt-best.meta\n",
            "INFO:tensorflow:saving model.ckpt-30.data-00000-of-00001 to model.ckpt-best.data-00000-of-00001\n",
            "I0109 00:39:04.168291 139639740966784 run_classifier.py:473] saving model.ckpt-30.data-00000-of-00001 to model.ckpt-best.data-00000-of-00001\n",
            "INFO:tensorflow:saving model.ckpt-30.index to model.ckpt-best.index\n",
            "I0109 00:39:04.974633 139639740966784 run_classifier.py:473] saving model.ckpt-30.index to model.ckpt-best.index\n",
            "cp gs://luanps/albert-tfhub/models/RTE/ddfccaa4/eval_results.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-09 00:39:06,572]\u001b[0m Trial 0 finished with value: 0.7545126 and parameters: {'warmup_steps': 10, 'train_steps': 30, 'learning_rate': 0.038339760285950646, 'batch_size': 80}. Best is trial 0 with value: 0.7545126.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "I0109 00:39:38.714227 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.715208 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.716257 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.717212 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.718232 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.719267 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.720203 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.721073 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.721985 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.722904 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.723742 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.724741 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.725849 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.726824 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.727678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.728547 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.729544 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.730462 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.731335 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.732195 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.733300 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.734486 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.735417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.736249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.737162 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.738051 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.739062 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.740172 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.741151 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.742053 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.742942 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.743816 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.744774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.745909 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.746995 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.748050 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.749039 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.749905 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.750917 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.751868 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.752731 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.753614 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.754475 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.755362 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.756281 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.757245 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.758256 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.759406 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.760607 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.762244 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.763756 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.765272 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.766703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.768062 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.769476 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.770805 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.772231 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.773614 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.775187 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.776746 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.778219 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.779689 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.781479 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.783367 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.785028 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.786508 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.787502 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.788565 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.789570 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.790495 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.791556 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.792585 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.793501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.794822 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.795966 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.796872 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.797915 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.798950 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.799943 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.800995 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.802057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.803040 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.804058 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.805075 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.805977 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.806866 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.807740 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.808657 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.809700 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.810668 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.811557 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.812487 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.813336 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.814254 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.815196 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.816066 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.817167 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.818186 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.819088 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.819963 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.820834 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.821730 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.822659 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.823725 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.824734 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.825608 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.826454 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.827482 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.828487 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.829384 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.830221 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.831077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.831957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.832875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.833804 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.834705 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.835674 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.836693 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.837709 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.838753 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.839844 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.840855 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.841886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.842930 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.843900 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.844797 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.845703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.846551 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.847432 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.848261 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.849141 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.850204 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.851209 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.852158 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.853249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.854200 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.855051 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.856021 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.856961 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.857860 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.858748 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.859808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.861058 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.862232 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.863184 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.864244 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.865262 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.866429 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.867957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.869091 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.870225 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.871167 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.872141 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.873445 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.875008 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.876258 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.877557 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.878604 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.879547 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.880963 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.882331 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.883619 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.884609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.885539 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.886495 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.887476 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.888351 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.889193 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.890082 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.891061 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.891952 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.892818 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.893709 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.894595 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.895488 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.896379 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.897255 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.898452 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.899695 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.900855 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.901886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.902759 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.903654 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.904571 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.905483 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.906384 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.907439 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.908334 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.909186 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.910263 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.911247 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.912089 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.912947 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.913959 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.914932 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.915784 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.916637 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.917495 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.918380 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.919244 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.920356 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.921386 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.922234 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.923283 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.924304 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.925156 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.926070 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.926926 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.927847 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.928794 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.929861 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.930916 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.931796 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.932672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.933548 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.934449 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.935376 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.936466 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.937425 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.938658 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.940027 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.941115 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.942243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.943246 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.944116 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.945462 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.946791 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.948760 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.950415 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.951916 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.952967 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.953866 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.954905 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.956044 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.957208 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.958501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.959609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.960519 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.961613 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.962601 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.963419 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.964231 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.965267 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.966445 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.967456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.968718 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.970068 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.971039 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.972453 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.973827 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.975219 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.976837 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.978417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.979812 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.981508 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.983052 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.984411 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.985721 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.987163 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.988609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.990280 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.991814 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.993006 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.994272 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.995876 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.997380 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:38.998757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.000342 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.001777 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.003186 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.004593 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.006100 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.007101 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.008424 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.009510 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.010514 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.011551 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.012606 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.013811 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.014851 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.015768 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.016758 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.017677 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.018523 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.019479 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.020524 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.021477 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.022389 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.023544 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.024626 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.025538 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.026443 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.027481 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.028538 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.029681 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.030667 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.031737 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.032713 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.033558 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.034451 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.035519 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.036538 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.037457 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.038501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.039449 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.040376 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.041460 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.042493 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.043589 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.044576 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.045794 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.046816 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.047726 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.048795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.049776 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.050647 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.051694 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.052864 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.054049 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.054934 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.055873 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.056823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.057890 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.058817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.059908 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.060922 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.061760 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.062699 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.063849 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.065103 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.066427 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.067583 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.068531 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.069556 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.071107 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.072870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.074438 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.075774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.076702 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.077762 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.078949 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.080057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.081550 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.083090 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.084598 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.085892 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.087322 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.088984 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.090555 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.091865 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.093273 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.094981 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.097581 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.099132 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.100694 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.102264 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.103837 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.107288 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.108694 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.110099 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.111545 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.112908 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.116239 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.117801 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.119296 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.120757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.122197 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.123604 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.125089 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.126464 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.127682 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.128604 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.129734 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.130790 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.131731 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.132782 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.133752 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.134953 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.136280 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.137401 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.138297 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.139213 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.140124 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.141046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.141931 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.143013 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.144059 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.145027 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.145907 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.146935 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.147884 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.148779 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.149697 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.150597 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.151559 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.152519 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.153407 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.154245 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.155099 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.156050 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.157026 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.157957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.158835 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.160055 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.161136 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.162083 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.162959 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.164011 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.164993 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.165920 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.166870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.167781 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.168662 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.169734 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.170653 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.171532 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.172715 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.174114 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.174999 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.175989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.176933 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.177771 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.178597 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.179663 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.180621 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.181521 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.182421 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.183435 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.184596 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.186051 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.187588 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.188794 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.189870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.191151 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.192438 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.193593 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.194921 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.196326 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.197961 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.199878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.201775 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.203023 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.203935 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.204823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.205694 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.206562 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.207449 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.208468 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.209419 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.210802 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.211892 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.212792 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.213804 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.214776 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.215683 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.216616 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.217790 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.218878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.219813 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.220791 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.221791 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.222947 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.223888 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.224944 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.225903 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.226923 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.228128 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.229459 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.230349 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.231161 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.232207 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.233387 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.234422 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.235425 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.236354 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.237382 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.238295 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.239366 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.240628 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.241682 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.242623 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.243499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.244763 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.246109 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.246973 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.247789 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.248620 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.249656 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.250609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.251693 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.252671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.253599 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.254548 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.255385 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.256432 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.257400 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.258451 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.259593 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.261115 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.262150 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.263251 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.264176 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.265089 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.266135 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.267111 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.268028 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.268954 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.270010 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.270984 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.272377 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.273797 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.275274 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.276716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.278057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.279297 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.280707 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.282386 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.283970 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.285499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.286951 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.288299 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.289453 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.290587 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.291757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.293324 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.294945 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.296383 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.297960 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.299504 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.301077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.302501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.303787 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.304747 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.305669 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.306607 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.307590 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.308682 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.309842 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.310827 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.311697 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.312757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.314120 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.315090 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.315988 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.316841 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.317716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.318809 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.319810 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.320651 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.321529 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.322521 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.323603 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.324556 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.325554 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.326481 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.327337 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.328218 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.329087 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.330022 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.331077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.332161 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.333119 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.333957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.334817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.335671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.336492 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.337321 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.338168 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.339046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.339919 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.340990 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.341936 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.342783 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.343706 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.344782 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.345945 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.347076 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.348046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.348921 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.349969 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.350964 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.351939 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.352881 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.353911 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.354875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.355772 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.356677 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.357723 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.358692 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.359519 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.360355 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.361239 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.362159 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.363011 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.363872 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.364774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.365644 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.366560 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.367465 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.368295 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.369130 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.369992 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.370873 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.371695 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.372516 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.373485 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.374478 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.375554 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.376836 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.377926 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.379027 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.380014 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.380894 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.381810 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.382761 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.383647 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.384645 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.385817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.386799 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.387684 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.388571 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.389469 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.390402 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.391264 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.392100 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.392971 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.393839 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.394671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.395535 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.396436 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.397413 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.398285 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.399450 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.400478 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.401321 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.402342 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.403347 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.404404 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.405539 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.406601 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.407677 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.408638 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.409628 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.410670 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.411558 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.412436 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.413346 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.414408 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.415522 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.416474 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.417379 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.418422 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.419418 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.420277 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.421163 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.422029 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.422879 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.423780 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.424710 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.425613 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.426532 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.427417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.428267 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.429451 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.430501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.431395 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.432249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.433577 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.434946 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.436299 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.437512 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.438499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.439591 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.441087 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.442276 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.443199 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.444334 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.445493 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.446720 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.447947 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.448932 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.449950 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.450886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.451772 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.452675 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.453573 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.454466 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.455338 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.456157 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.457012 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.457929 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.458881 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.459814 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.460878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.461800 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.462673 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.463527 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.464356 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.465168 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.466043 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.466972 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.467821 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.468685 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.469596 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.470499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.471403 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.472283 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.473289 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.474404 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.475639 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.477335 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.478847 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.480278 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.481721 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.483103 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.484501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.485842 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.487233 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.488652 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.490298 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.492017 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.493480 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.494651 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.495823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.497246 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.499009 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.500597 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.502296 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.503906 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.505511 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.506941 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.508276 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.509139 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.509971 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.510928 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.511991 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.512937 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.513893 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.514986 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.515932 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.516756 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.517605 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.518429 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.519329 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.520227 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.521160 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.522371 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.523413 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.524341 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.525364 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.526330 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.527175 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.528220 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.529330 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.530241 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.531136 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.532027 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.532937 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.533790 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.534627 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.535591 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.536566 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.537485 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.538380 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.539236 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.540169 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.540998 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.541810 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.542699 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.543813 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.544847 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.545983 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.546964 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.547905 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.548827 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.549703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.550562 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.551426 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.552269 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.553143 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.554028 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.554957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.555835 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.556772 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.558032 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.559070 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.560023 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.560914 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.561829 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.562781 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.563710 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.564685 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.565608 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.566517 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.567540 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.568434 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.569449 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.570475 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.571412 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.572276 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.573112 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.573986 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.574907 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.575819 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.576679 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.577573 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.578507 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.579699 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.581003 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.581915 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.582958 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.583931 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.584833 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.585738 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.586604 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.587534 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.588628 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.589749 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.590727 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.591669 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.592606 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.593523 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.594429 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.595351 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.596206 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.597355 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.598690 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.600304 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.601873 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.603138 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.604751 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.606838 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.608390 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.609852 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.610973 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.611807 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.612693 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.613596 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.614501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.615465 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.616369 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.617349 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.618261 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.619142 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.620184 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.621211 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.622223 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.623170 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.624126 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.625097 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.625989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.627081 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.628068 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.629141 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.630048 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.630903 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.631724 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.632805 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.633961 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.634934 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.635842 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.636797 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.637667 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.638713 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.639700 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.640580 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.641430 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.642278 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.643159 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.644082 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.645025 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.645902 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.646760 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.648028 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.649079 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.650053 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.651174 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.652209 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.653062 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.653995 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.655057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.656159 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.657147 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.658057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.658960 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.659880 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.660745 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.661654 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.662781 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.663835 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.664774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.665841 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.666877 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.667889 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.668929 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.669955 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.670914 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.671803 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.672675 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.673588 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.674454 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.675393 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.676255 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.677180 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.678243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.679243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.680298 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.681682 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.683053 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.684520 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.686023 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.687453 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.688882 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.690330 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.691707 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.693197 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.694654 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.696015 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.699088 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.700822 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.702182 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.703581 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.705086 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.706575 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.708205 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.709798 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.711232 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.712561 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.714386 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.715795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.716808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.717693 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.718611 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.719554 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.720482 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.721462 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.722560 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.723705 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.724703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.725599 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.726466 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.727586 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.728754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.729724 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.730606 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.731456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.732294 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.733229 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.734148 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.735087 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.736271 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.737161 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.738023 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.738909 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.739942 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.741097 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.742114 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.743238 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.744404 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.745304 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.746112 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.746975 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.747834 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.748672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.749559 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.750753 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.751848 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.753157 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.754395 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.755472 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.756392 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.757456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.758456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.759333 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.760169 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.761281 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.762382 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.763249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.764071 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.764950 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.765822 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.766795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.767719 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.768639 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.769509 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.770488 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.771370 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.772229 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.773241 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.774215 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.775157 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.775991 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.776842 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.777710 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.778622 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.779505 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.780389 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.781363 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.782241 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.783147 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.784493 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.785742 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.786843 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.787766 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.788694 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.789695 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.790837 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.791822 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.792891 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.793889 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.794811 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.795740 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.796719 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.797901 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.798870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.799727 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.800613 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.801624 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.802578 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.803613 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.804710 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.805855 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.806741 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.808130 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.809242 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.810125 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.811004 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.811878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.812774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.813664 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.814524 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.815376 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.816260 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.817150 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.818010 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.818988 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.819930 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.821075 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.822302 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.823521 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.824577 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.825442 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.826289 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.827150 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.828072 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.829013 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.829961 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.830870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.831773 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.832808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.833743 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.834631 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.835571 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.836432 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.837322 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.838252 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.839158 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.840017 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.840875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.841766 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.842688 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.843600 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.844450 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.845270 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.847271 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.848630 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.849999 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.850927 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.851825 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.852758 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.853798 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.855017 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.856065 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.857046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.858021 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.858924 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.859846 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.860901 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.861970 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.862837 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.863840 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.864820 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.865722 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.866808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.867803 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.868747 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.869671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.870651 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.871550 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.872445 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.873598 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.874659 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.875587 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.876615 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.877616 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.878526 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.879567 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.880545 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.881685 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.885165 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.886479 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.887910 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.889350 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.890864 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.892261 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.893634 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.895210 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.896924 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.898757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.900292 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.901430 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.902638 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.903931 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.905339 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.906673 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.908086 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.909598 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.911541 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.913262 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.915055 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.916257 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.917208 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.918092 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.918989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.919886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.920788 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.921750 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.922688 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.923635 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.924563 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.925437 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.926348 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.927167 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.928151 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.929077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.929927 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.930768 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.931612 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.932432 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.933243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.934060 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.934892 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.935777 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.936739 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.937641 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.938569 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.939635 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.940593 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.941450 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.942400 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.943286 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.944116 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.945008 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.946019 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.946887 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.947859 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.948907 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.949855 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.950729 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.951600 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.952477 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.953294 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.954122 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.954926 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.955934 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.956886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.957764 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.958626 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.959428 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.960493 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.961457 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.962446 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.963371 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.964336 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.965568 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.966598 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.967657 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.968622 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.969465 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.970268 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.971158 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.972322 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.973221 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.974133 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.975012 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.976004 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.977200 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.978661 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.980343 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.982109 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.983450 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.984875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.986512 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.987532 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.988494 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.989609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.990557 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.991532 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.992743 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.993643 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.994502 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.995795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.997268 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.998239 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:39.999186 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.000319 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.001414 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.002348 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.003213 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.004101 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.005231 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.006162 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.007126 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.008125 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.009058 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.009948 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.010860 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.011729 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.012657 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.013816 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.014800 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.015649 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.016578 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.017555 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.018471 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.019330 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.020215 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.021249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.022224 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.023096 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.023940 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.024836 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.025890 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.026972 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.027876 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.028763 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.029697 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.030608 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.031584 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.032756 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.033814 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.034716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.035593 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.036523 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.037618 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.038569 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.039402 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.040251 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.041214 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.042248 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.043420 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.044471 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.045296 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.046150 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.047004 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.048024 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.048891 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.049762 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.050671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.051520 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.052526 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.053527 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.054680 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.055761 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.056681 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.057571 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.058458 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.059362 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.060271 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.061158 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.062182 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.063099 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.063987 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.064984 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.065954 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.067005 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.067965 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.068836 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.069845 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.070878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.072160 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.073532 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.074853 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.075754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.076663 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.077534 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.078440 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.079689 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.080775 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.081747 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.082665 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.083592 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.084614 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.085502 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.086285 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.087146 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.088057 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.088943 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.089826 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.090764 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.091611 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.092433 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.093278 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.094127 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.094989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.096555 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.098076 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.099499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.100786 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.102235 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.103699 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.105071 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.106482 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.107942 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.109398 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.110769 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.112365 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.114336 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.115970 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.117621 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.119190 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.120808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.122418 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.123738 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.124944 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.126171 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.127479 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.129094 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.130481 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.131865 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.133243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.134926 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.136441 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.137754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.139102 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.140441 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.141887 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.143306 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.145183 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.146924 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.148739 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.149961 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.150883 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.151753 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.152812 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.154035 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.155041 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.156159 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.157158 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.158104 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.158988 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.159868 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.160879 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.161915 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.162894 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.163816 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.164725 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.165975 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.167067 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.167944 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.168849 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.169748 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.170727 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.171747 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.172670 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.173643 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.174704 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.175671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.176713 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.177667 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.178588 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.179491 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.180608 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.181823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.182922 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.183917 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.184775 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.185664 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.186547 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.187397 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.188220 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.189096 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.189948 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.190758 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.191809 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.192804 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.193756 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.194679 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.195576 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.196481 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.197360 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.198627 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.199827 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.200743 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.201757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.202775 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.203927 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.204954 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.205791 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.206716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.207607 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.208619 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.209587 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.210448 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.211417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.212328 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.213410 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.214408 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.215206 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.216017 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.216866 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.217720 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.218568 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.219417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.220270 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.221159 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.222273 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.223757 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.224777 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.225774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.226859 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.227815 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.228683 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.229678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.231143 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.232074 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.232985 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.233896 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.234809 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.235835 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.237007 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.238035 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.238968 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.239874 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.240761 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.241599 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.242480 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.243575 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.244581 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.245790 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.246676 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.247540 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.248395 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.249459 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.250473 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.251392 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.252253 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.253085 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.254100 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.255120 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.256034 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.256993 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.258054 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.259231 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.260324 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.261259 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.262161 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.263205 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.264218 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.265337 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.266333 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.267204 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.268077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.268966 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.269865 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.270813 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.271772 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.272665 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.273577 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.274538 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.275408 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.276235 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.277074 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.277946 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.278777 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.279703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.280603 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.281493 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.282486 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.283488 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.284496 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.285412 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.286228 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.287191 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.288070 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.288989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.289935 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.290839 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.291716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.292725 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.293732 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.294634 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.295504 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.296494 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.297398 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.298241 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.299242 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.300703 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.301791 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.302774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.303931 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.305077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.306691 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.308131 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.309683 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.311174 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.312833 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.314456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.315853 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.317282 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.319040 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.320585 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.321934 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.323257 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.324644 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.326210 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.327714 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.329288 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.330873 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.332181 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.333475 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.334795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.336084 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.337081 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.338029 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.338943 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.339866 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.340827 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.341709 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.342626 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.343700 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.344733 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.345871 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.346879 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.347918 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.348935 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.349859 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.350760 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.351639 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.352546 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.353408 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.354243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.355088 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.355972 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.356901 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.357797 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.358645 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.359501 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.360334 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.361177 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.362080 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.362957 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.364038 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.365092 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.366021 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.366990 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.367951 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.368888 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.369842 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.370732 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.371558 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.372465 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.373439 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.374505 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.375470 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.376302 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.377175 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.378046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.379144 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.380271 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.381230 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.382136 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.383039 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.383885 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.384864 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.385946 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.387068 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.387989 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.388760 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.389783 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.390875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.391809 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.392699 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.393583 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.394433 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.395279 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.396283 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.397214 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.398098 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.399015 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.400127 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.401350 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.403073 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.404357 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.405257 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.406291 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.407263 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.408160 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.409079 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.409935 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.410972 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.412081 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.413107 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.414035 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.414896 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.415726 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.416576 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.417671 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.418676 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.419597 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.420504 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.421398 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.422302 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.423234 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.424133 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.425071 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.425958 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.427040 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.428034 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.428903 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.429776 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.430609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.431429 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.432237 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.433127 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.434071 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.434944 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.435813 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.436701 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.437625 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.438678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.439815 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.440844 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.441872 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.442812 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.443659 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.444617 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.445509 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.446444 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.447579 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.448734 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.449882 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.450885 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.451871 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.452854 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.453774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.454661 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.455771 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.456806 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.457717 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.458651 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.459820 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.460721 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.461619 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.462544 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.463408 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.464187 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.465050 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.465983 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.467007 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.468228 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.469265 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.470170 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.471094 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.471958 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.472969 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.473998 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.474894 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.475904 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.476856 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.477728 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.478638 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.479520 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.480437 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.481559 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.482609 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.483615 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.484686 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.485614 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.486495 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.487367 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.488397 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.489394 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.490259 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.491132 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.492034 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.492968 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.493968 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.494878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.495704 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.496553 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.497421 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.498745 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.500087 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.501656 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.502901 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.503915 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.505245 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.506672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.507828 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.509252 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.510602 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.511589 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.512583 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.513510 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.514504 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.515433 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.516294 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.517258 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.518179 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.519076 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.520131 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.521094 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.521908 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.522880 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.523821 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.524672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.525531 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.526368 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.527339 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.528229 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.529046 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.529896 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.530773 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.531683 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.532691 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.533618 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.534617 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.535573 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.536491 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.537425 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.538376 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.539242 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.540093 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.541026 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.542010 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.542898 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.543750 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.544651 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.545525 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.546469 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.547303 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.548173 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.549038 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.550205 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.551304 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.552250 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.553302 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.554325 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.555219 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.556231 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.557131 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.558058 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.559190 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.560189 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.561053 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.562074 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.563088 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.563998 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.564930 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.565808 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.566883 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.567878 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.568781 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.569668 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.570580 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.571620 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.572575 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.573425 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.574303 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.575188 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.576023 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.576894 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.577795 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.578720 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.579610 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.580485 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.581404 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.582336 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.583202 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.584142 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.585059 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.585958 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.587041 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.588070 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.588996 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.589880 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.590739 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.591598 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.592641 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.593682 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.594575 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.595481 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.596560 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.597559 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.598486 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.599378 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.600464 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.601433 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.602500 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.603923 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.605182 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.606453 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.607736 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.609074 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.610850 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.612756 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.614456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.616029 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.617453 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.618886 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.620109 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.621230 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.622775 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.624711 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.626509 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.627985 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.629347 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.630864 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.632388 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.634075 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.635716 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.637175 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.638737 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.639993 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.641020 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.641953 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.642992 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.644053 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.645144 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.646129 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.647110 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.648038 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.648884 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.649965 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.650982 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.652052 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:39:40.653018 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:***** Running training *****\n",
            "I0109 00:39:41.539478 139651685500800 run_classifier.py:315] ***** Running training *****\n",
            "INFO:tensorflow:  Num examples = 2490\n",
            "I0109 00:39:41.539795 139651685500800 run_classifier.py:316]   Num examples = 2490\n",
            "INFO:tensorflow:  Batch size = 112\n",
            "I0109 00:39:41.539936 139651685500800 run_classifier.py:317]   Batch size = 112\n",
            "INFO:tensorflow:  Num steps = 60\n",
            "I0109 00:39:41.540009 139651685500800 run_classifier.py:318]   Num steps = 60\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.183.170:8470) for TPU system metadata.\n",
            "I0109 00:39:41.794348 139651685500800 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.76.183.170:8470) for TPU system metadata.\n",
            "2022-01-09 00:39:41.795562: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "I0109 00:39:41.807211 139651685500800 tpu_system_metadata.py:148] Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "I0109 00:39:41.807481 139651685500800 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "I0109 00:39:41.807611 139651685500800 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "I0109 00:39:41.807924 139651685500800 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11427571705680393491)\n",
            "I0109 00:39:41.808072 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11427571705680393491)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11027356195149920114)\n",
            "I0109 00:39:41.808346 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11027356195149920114)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9365630759094707857)\n",
            "I0109 00:39:41.808441 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9365630759094707857)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7989586621220768446)\n",
            "I0109 00:39:41.808526 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7989586621220768446)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9785832277138233337)\n",
            "I0109 00:39:41.808609 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9785832277138233337)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9613850914372366153)\n",
            "I0109 00:39:41.808688 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9613850914372366153)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7293297392476955861)\n",
            "I0109 00:39:41.808789 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7293297392476955861)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6142064186121730076)\n",
            "I0109 00:39:41.808872 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6142064186121730076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3940765502806562279)\n",
            "I0109 00:39:41.808953 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3940765502806562279)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5185886070249345043)\n",
            "I0109 00:39:41.809034 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5185886070249345043)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18302436322667190563)\n",
            "I0109 00:39:41.809118 139651685500800 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18302436322667190563)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "W0109 00:39:41.814140 139651685500800 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0109 00:39:41.814657 139651685500800 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:39:41.828475 139651685500800 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:744: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0109 00:39:41.850396 139651685500800 deprecation.py:323] From /content/albert/classifier_utils.py:744: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0109 00:39:41.850611 139651685500800 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b6b87050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:39:41.862262 139651685500800 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b6b87050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:721: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0109 00:39:41.865658 139651685500800 deprecation.py:323] From /content/albert/classifier_utils.py:721: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:39:41.935191 139651685500800 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (14, 512)\n",
            "I0109 00:39:41.935459 139651685500800 classifier_utils.py:826]   name = input_ids, shape = (14, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (14, 512)\n",
            "I0109 00:39:41.935542 139651685500800 classifier_utils.py:826]   name = input_mask, shape = (14, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (14,)\n",
            "I0109 00:39:41.935609 139651685500800 classifier_utils.py:826]   name = is_real_example, shape = (14,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (14,)\n",
            "I0109 00:39:41.935670 139651685500800 classifier_utils.py:826]   name = label_ids, shape = (14,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (14, 512)\n",
            "I0109 00:39:41.935732 139651685500800 classifier_utils.py:826]   name = segment_ids, shape = (14, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:39:41.936610 139651685500800 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:39:46.478083 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.478740 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.479780 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.480163 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.483561 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.489780 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.495570 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.498457 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.499574 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.507409 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.508579 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.519026 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.520800 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.529466 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.531157 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.540808 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.542488 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.560462 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.562131 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.568137 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.569256 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.575637 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.577109 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.586680 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.587797 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.592978 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:46.594054 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.296780 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.298007 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.305011 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.306133 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.310036 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.311156 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:39:47.315245 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:39:47.416263 139651685500800 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From /content/albert/classifier_utils.py:794: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0109 00:39:47.536746 139651685500800 deprecation.py:506] From /content/albert/classifier_utils.py:794: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:39:47.572807 139651685500800 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:39:47.573073 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:39:47.573216 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:39:47.573344 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:39:47.573580 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:39:47.573690 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:39:47.573811 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:39:47.573937 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:39:47.574046 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:39:47.574158 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:39:47.574263 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:39:47.574392 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:39:47.574506 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:39:47.574614 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:39:47.574713 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:39:47.574819 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:39:47.574924 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:39:47.575023 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:39:47.575119 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:39:47.575224 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:39:47.575338 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:39:47.575455 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:39:47.575563 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:39:47.575663 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:39:47.575759 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:39:47.575868 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:39:47.575967 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:39:47.576071 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:39:47.576167 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:39:47.576262 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:39:47.576369 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:39:47.576467 139651685500800 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:39:47.576579 139651685500800 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:++++++ warmup starts at step 0, for 5 steps ++++++\n",
            "I0109 00:39:47.588765 139651685500800 optimization.py:51] ++++++ warmup starts at step 0, for 5 steps ++++++\n",
            "INFO:tensorflow:using adamw\n",
            "I0109 00:39:47.600898 139651685500800 optimization.py:75] using adamw\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0109 00:39:47.811135 139651685500800 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0109 00:39:52.965109 139651685500800 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:39:53.033277 139651685500800 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:39:56.158476 139651685500800 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:39:56.374687 139651685500800 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:40:02.831974 139651685500800 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:40:03.385817 139651685500800 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt.\n",
            "I0109 00:40:11.768919 139651685500800 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt.\n",
            "INFO:tensorflow:gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "I0109 00:40:20.131035 139651685500800 checkpoint_management.py:95] gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "W0109 00:40:24.340439 139651685500800 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:40:25.521167 139651685500800 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "I0109 00:40:25.521614 139651685500800 session_support.py:332] Installing graceful shutdown hook.\n",
            "2022-01-09 00:40:25.522005: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0109 00:40:25.527029 139651685500800 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0109 00:40:25.528907 139651685500800 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:40:25.532556 139651685500800 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 18 seconds\n",
            "I0109 00:40:43.875936 139651685500800 tpu_estimator.py:576] Initialized TPU in 18 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:40:43.876721 139649554188032 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:40:43.877117 139649545795328 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (60) batch(es) of data to infeed.\n",
            "I0109 00:40:44.433749 139651685500800 tpu_estimator.py:600] Enqueue next (60) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (60) batch(es) of data from outfeed.\n",
            "I0109 00:40:44.434129 139651685500800 tpu_estimator.py:604] Dequeue next (60) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:41:02.621380 139649545795328 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:loss = 115.106995, step = 60\n",
            "I0109 00:41:28.910454 139651685500800 basic_session_run_hooks.py:262] loss = 115.106995, step = 60\n",
            "INFO:tensorflow:Saving checkpoints for 60 into gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt.\n",
            "I0109 00:41:28.912283 139651685500800 basic_session_run_hooks.py:606] Saving checkpoints for 60 into gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt.\n",
            "INFO:tensorflow:gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "I0109 00:41:38.114603 139651685500800 checkpoint_management.py:95] gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:41:42.743520 139651685500800 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:41:42.743880 139651685500800 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:41:42.744198 139649554188032 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:41:42.744370 139649554188032 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:41:42.744767 139651685500800 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:41:42.744941 139651685500800 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:41:42.745041 139651685500800 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:41:42.745202 139649545795328 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:41:42.745302 139649545795328 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:41:42.745593 139651685500800 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:41:42.745731 139651685500800 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 115.106995.\n",
            "I0109 00:41:43.567712 139651685500800 estimator.py:371] Loss for final step: 115.106995.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "I0109 00:41:43.568201 139651685500800 error_handling.py:101] training_loop marked as finished\n",
            "INFO:tensorflow:Writing example 0 of 280\n",
            "I0109 00:41:43.847805 139651685500800 classifier_utils.py:671] Writing example 0 of 280\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:43.848407 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:41:43.848901 139651685500800 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 0\n",
            "I0109 00:41:43.849000 139651685500800 classifier_utils.py:646] guid: 0\n",
            "INFO:tensorflow:tokens: [CLS] ▁dana ▁reeve , ▁the ▁widow ▁of ▁the ▁actor ▁christopher ▁reeve , ▁has ▁died ▁of ▁lung ▁cancer ▁at ▁age ▁44 , ▁according ▁to ▁the ▁christopher ▁reeve ▁foundation . [SEP] ▁christopher ▁reeve ▁had ▁an ▁accident . [SEP]\n",
            "I0109 00:41:43.849114 139651685500800 classifier_utils.py:648] tokens: [CLS] ▁dana ▁reeve , ▁the ▁widow ▁of ▁the ▁actor ▁christopher ▁reeve , ▁has ▁died ▁of ▁lung ▁cancer ▁at ▁age ▁44 , ▁according ▁to ▁the ▁christopher ▁reeve ▁foundation . [SEP] ▁christopher ▁reeve ▁had ▁an ▁accident . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 11478 24604 15 14 5151 16 14 1574 4479 24604 15 63 440 16 9223 2637 35 348 4576 15 496 20 14 4479 24604 1304 9 3 4479 24604 41 40 3351 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.849423 139651685500800 classifier_utils.py:649] input_ids: 2 11478 24604 15 14 5151 16 14 1574 4479 24604 15 63 440 16 9223 2637 35 348 4576 15 496 20 14 4479 24604 1304 9 3 4479 24604 41 40 3351 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.849706 139651685500800 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.849996 139651685500800 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:41:43.850082 139651685500800 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:43.851267 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:41:43.851808 139651685500800 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 1\n",
            "I0109 00:41:43.851915 139651685500800 classifier_utils.py:646] guid: 1\n",
            "INFO:tensorflow:tokens: [CLS] ▁yet , ▁we ▁now ▁are ▁discovering ▁that ▁antibiotic s ▁are ▁losing ▁their ▁effectiveness ▁against ▁illness . ▁disease - ca using ▁bacteria ▁are ▁muta ting ▁faster ▁than ▁we ▁can ▁come ▁up ▁with ▁new ▁antibiotic s ▁to ▁fight ▁the ▁new ▁variations . [SEP] ▁bacteria ▁is ▁winning ▁the ▁war ▁against ▁antibiotic s . [SEP]\n",
            "I0109 00:41:43.852023 139651685500800 classifier_utils.py:648] tokens: [CLS] ▁yet , ▁we ▁now ▁are ▁discovering ▁that ▁antibiotic s ▁are ▁losing ▁their ▁effectiveness ▁against ▁illness . ▁disease - ca using ▁bacteria ▁are ▁muta ting ▁faster ▁than ▁we ▁can ▁come ▁up ▁with ▁new ▁antibiotic s ▁to ▁fight ▁the ▁new ▁variations . [SEP] ▁bacteria ▁is ▁winning ▁the ▁war ▁against ▁antibiotic s . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 768 15 95 130 50 15799 30 20017 18 50 2281 66 14115 149 6761 9 2515 8 793 12655 10955 50 22673 1203 4233 119 95 92 340 71 29 78 20017 18 20 1074 14 78 8194 9 3 10955 25 1414 14 176 149 20017 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.852298 139651685500800 classifier_utils.py:649] input_ids: 2 768 15 95 130 50 15799 30 20017 18 50 2281 66 14115 149 6761 9 2515 8 793 12655 10955 50 22673 1203 4233 119 95 92 340 71 29 78 20017 18 20 1074 14 78 8194 9 3 10955 25 1414 14 176 149 20017 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.852759 139651685500800 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.853019 139651685500800 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: entailment (id = 0)\n",
            "I0109 00:41:43.853107 139651685500800 classifier_utils.py:652] label: entailment (id = 0)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:43.854410 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:41:43.854924 139651685500800 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 2\n",
            "I0109 00:41:43.855006 139651685500800 classifier_utils.py:646] guid: 2\n",
            "INFO:tensorflow:tokens: [CLS] ▁cairo ▁is ▁now ▁home ▁to ▁some ▁15 ▁million ▁people ▁ - ▁a ▁burgeoning ▁population ▁that ▁produces ▁approximately ▁10,000 ▁tonnes ▁of ▁rubbish ▁per ▁day , ▁putting ▁an ▁enormous ▁strain ▁on ▁public ▁services . ▁in ▁the ▁past ▁10 ▁years , ▁the ▁government ▁has ▁tried ▁hard ▁to ▁encourage ▁private ▁investment ▁in ▁the ▁refuse ▁sector , ▁but ▁some ▁estimate ▁4,000 ▁tonnes ▁of ▁waste ▁is ▁left ▁behind ▁every ▁day , ▁fest ering ▁in ▁the ▁heat ▁as ▁it ▁wait s ▁for ▁someone ▁to ▁clear ▁it ▁up . ▁it ▁is ▁often ▁the ▁people ▁in ▁the ▁poor est ▁neighbourhood s ▁that ▁are ▁worst ▁affected . ▁but ▁in ▁some ▁areas ▁they ▁are ▁fighting ▁back . ▁in ▁shu bra , ▁one ▁of ▁the ▁northern ▁districts ▁of ▁the ▁city , ▁the ▁residents ▁have ▁taken ▁to ▁the ▁streets ▁armed ▁with ▁dust pan s ▁and ▁brushes ▁to ▁clean ▁up ▁public ▁areas ▁which ▁have ▁been ▁used ▁as ▁public ▁dump s . [SEP] ▁15 ▁million ▁tonnes ▁of ▁rubbish ▁are ▁produced ▁daily ▁in ▁cairo . [SEP]\n",
            "I0109 00:41:43.855162 139651685500800 classifier_utils.py:648] tokens: [CLS] ▁cairo ▁is ▁now ▁home ▁to ▁some ▁15 ▁million ▁people ▁ - ▁a ▁burgeoning ▁population ▁that ▁produces ▁approximately ▁10,000 ▁tonnes ▁of ▁rubbish ▁per ▁day , ▁putting ▁an ▁enormous ▁strain ▁on ▁public ▁services . ▁in ▁the ▁past ▁10 ▁years , ▁the ▁government ▁has ▁tried ▁hard ▁to ▁encourage ▁private ▁investment ▁in ▁the ▁refuse ▁sector , ▁but ▁some ▁estimate ▁4,000 ▁tonnes ▁of ▁waste ▁is ▁left ▁behind ▁every ▁day , ▁fest ering ▁in ▁the ▁heat ▁as ▁it ▁wait s ▁for ▁someone ▁to ▁clear ▁it ▁up . ▁it ▁is ▁often ▁the ▁people ▁in ▁the ▁poor est ▁neighbourhood s ▁that ▁are ▁worst ▁affected . ▁but ▁in ▁some ▁areas ▁they ▁are ▁fighting ▁back . ▁in ▁shu bra , ▁one ▁of ▁the ▁northern ▁districts ▁of ▁the ▁city , ▁the ▁residents ▁have ▁taken ▁to ▁the ▁streets ▁armed ▁with ▁dust pan s ▁and ▁brushes ▁to ▁clean ▁up ▁public ▁areas ▁which ▁have ▁been ▁used ▁as ▁public ▁dump s . [SEP] ▁15 ▁million ▁tonnes ▁of ▁rubbish ▁are ▁produced ▁daily ▁in ▁cairo . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 11772 25 130 213 20 109 357 507 148 13 8 21 29936 350 30 6700 1357 6693 11485 16 28898 416 208 15 3873 40 7135 8302 27 317 687 9 19 14 640 332 122 15 14 283 63 794 552 20 8333 932 3709 19 14 10198 3172 15 47 109 10243 13845 11485 16 4600 25 225 439 352 208 15 11837 7882 19 14 1737 28 32 1760 18 26 737 20 1207 32 71 9 32 25 478 14 148 19 14 1696 1430 9876 18 30 50 4126 4114 9 47 19 109 924 59 50 1849 97 9 19 5728 2559 15 53 16 14 743 3872 16 14 136 15 14 2175 57 658 20 14 3240 2799 29 4346 3206 18 17 25079 20 2745 71 317 924 56 57 74 147 28 317 11424 18 9 3 357 507 11485 16 28898 50 671 1954 19 11772 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.942789 139651685500800 classifier_utils.py:649] input_ids: 2 11772 25 130 213 20 109 357 507 148 13 8 21 29936 350 30 6700 1357 6693 11485 16 28898 416 208 15 3873 40 7135 8302 27 317 687 9 19 14 640 332 122 15 14 283 63 794 552 20 8333 932 3709 19 14 10198 3172 15 47 109 10243 13845 11485 16 4600 25 225 439 352 208 15 11837 7882 19 14 1737 28 32 1760 18 26 737 20 1207 32 71 9 32 25 478 14 148 19 14 1696 1430 9876 18 30 50 4126 4114 9 47 19 109 924 59 50 1849 97 9 19 5728 2559 15 53 16 14 743 3872 16 14 136 15 14 2175 57 658 20 14 3240 2799 29 4346 3206 18 17 25079 20 2745 71 317 924 56 57 74 147 28 317 11424 18 9 3 357 507 11485 16 28898 50 671 1954 19 11772 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.943238 139651685500800 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.943537 139651685500800 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:41:43.943644 139651685500800 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:43.944922 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:41:43.945381 139651685500800 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 3\n",
            "I0109 00:41:43.945497 139651685500800 classifier_utils.py:646] guid: 3\n",
            "INFO:tensorflow:tokens: [CLS] ▁the ▁a mish ▁community ▁in ▁pennsylvania , ▁which ▁numbers ▁about ▁5 5,000 , ▁lives ▁an ▁agrarian ▁lifestyle , ▁shun ning ▁technological ▁advances ▁like ▁electricity ▁and ▁automobile s . ▁and ▁many ▁say ▁their ▁in s ular ▁lifestyle ▁gives ▁them ▁a ▁sense ▁that ▁they ▁are ▁protected ▁from ▁the ▁violence ▁of ▁american ▁society . ▁but ▁as ▁residents ▁gathered ▁near ▁the ▁school , ▁some ▁wearing ▁traditional ▁garb ▁and ▁arriving ▁in ▁horse - drawn ▁bug gies , ▁they ▁said ▁that ▁sense ▁of ▁safety ▁had ▁been ▁shattered . ▁ \" if ▁someone ▁snap s ▁and ▁wants ▁to ▁do ▁something ▁stupid , ▁there ' s ▁no ▁distance ▁that ' s ▁going ▁to ▁stop ▁them , \" ▁said ▁jake ▁king , ▁56 , ▁an ▁a mish ▁lantern ▁maker ▁who ▁knew ▁several ▁families ▁whose ▁children ▁had ▁been ▁shot . [SEP] ▁pennsylvania ▁has ▁the ▁biggest ▁a mish ▁community ▁in ▁the ▁u . s . [SEP]\n",
            "I0109 00:41:43.945641 139651685500800 classifier_utils.py:648] tokens: [CLS] ▁the ▁a mish ▁community ▁in ▁pennsylvania , ▁which ▁numbers ▁about ▁5 5,000 , ▁lives ▁an ▁agrarian ▁lifestyle , ▁shun ning ▁technological ▁advances ▁like ▁electricity ▁and ▁automobile s . ▁and ▁many ▁say ▁their ▁in s ular ▁lifestyle ▁gives ▁them ▁a ▁sense ▁that ▁they ▁are ▁protected ▁from ▁the ▁violence ▁of ▁american ▁society . ▁but ▁as ▁residents ▁gathered ▁near ▁the ▁school , ▁some ▁wearing ▁traditional ▁garb ▁and ▁arriving ▁in ▁horse - drawn ▁bug gies , ▁they ▁said ▁that ▁sense ▁of ▁safety ▁had ▁been ▁shattered . ▁ \" if ▁someone ▁snap s ▁and ▁wants ▁to ▁do ▁something ▁stupid , ▁there ' s ▁no ▁distance ▁that ' s ▁going ▁to ▁stop ▁them , \" ▁said ▁jake ▁king , ▁56 , ▁an ▁a mish ▁lantern ▁maker ▁who ▁knew ▁several ▁families ▁whose ▁children ▁had ▁been ▁shot . [SEP] ▁pennsylvania ▁has ▁the ▁biggest ▁a mish ▁community ▁in ▁the ▁u . s . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 14 21 10758 514 19 1806 15 56 2116 88 331 5157 15 1551 40 24463 8937 15 15800 2981 10381 10592 101 5592 17 8199 18 9 17 151 395 66 19 18 7451 8937 2352 105 21 1259 30 59 50 3803 37 14 3300 16 189 641 9 47 28 2175 4744 424 14 116 15 109 2466 1361 19646 17 6426 19 1729 8 19950 6256 19008 15 59 87 30 1259 16 2108 41 74 11915 9 13 7 821 737 6877 18 17 2846 20 107 301 3553 15 80 22 18 90 1583 30 22 18 228 20 747 105 15 7 87 3777 437 15 6171 15 40 21 10758 11585 11767 72 404 238 1250 1196 391 41 74 999 9 3 1806 63 14 3835 21 10758 514 19 14 287 9 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.945865 139651685500800 classifier_utils.py:649] input_ids: 2 14 21 10758 514 19 1806 15 56 2116 88 331 5157 15 1551 40 24463 8937 15 15800 2981 10381 10592 101 5592 17 8199 18 9 17 151 395 66 19 18 7451 8937 2352 105 21 1259 30 59 50 3803 37 14 3300 16 189 641 9 47 28 2175 4744 424 14 116 15 109 2466 1361 19646 17 6426 19 1729 8 19950 6256 19008 15 59 87 30 1259 16 2108 41 74 11915 9 13 7 821 737 6877 18 17 2846 20 107 301 3553 15 80 22 18 90 1583 30 22 18 228 20 747 105 15 7 87 3777 437 15 6171 15 40 21 10758 11585 11767 72 404 238 1250 1196 391 41 74 999 9 3 1806 63 14 3835 21 10758 514 19 14 287 9 18 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:43.946079 139651685500800 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:44.043692 139651685500800 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: not_entailment (id = 1)\n",
            "I0109 00:41:44.044301 139651685500800 classifier_utils.py:652] label: not_entailment (id = 1)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.046487 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0109 00:41:44.046848 139651685500800 classifier_utils.py:645] *** Example ***\n",
            "INFO:tensorflow:guid: 4\n",
            "I0109 00:41:44.046976 139651685500800 classifier_utils.py:646] guid: 4\n",
            "INFO:tensorflow:tokens: [CLS] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁an ▁election ▁campaign ▁in ▁which ▁more ▁than ▁1,000 ▁people , ▁including ▁seven ▁election ▁candidates , ▁have ▁been ▁killed . [SEP] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁a ▁campaign ▁marred ▁by ▁violence . [SEP]\n",
            "I0109 00:41:44.047088 139651685500800 classifier_utils.py:648] tokens: [CLS] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁an ▁election ▁campaign ▁in ▁which ▁more ▁than ▁1,000 ▁people , ▁including ▁seven ▁election ▁candidates , ▁have ▁been ▁killed . [SEP] ▁security ▁forces ▁were ▁on ▁high ▁alert ▁after ▁a ▁campaign ▁marred ▁by ▁violence . [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1221 879 46 27 183 7863 75 40 776 1150 19 56 91 119 5925 148 15 215 810 776 4074 15 57 74 841 9 3 1221 879 46 27 183 7863 75 21 1150 26479 34 3300 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:44.047298 139651685500800 classifier_utils.py:649] input_ids: 2 1221 879 46 27 183 7863 75 40 776 1150 19 56 91 119 5925 148 15 215 810 776 4074 15 57 74 841 9 3 1221 879 46 27 183 7863 75 21 1150 26479 34 3300 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:44.047548 139651685500800 classifier_utils.py:650] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0109 00:41:44.047759 139651685500800 classifier_utils.py:651] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: entailment (id = 0)\n",
            "I0109 00:41:44.047849 139651685500800 classifier_utils.py:652] label: entailment (id = 0)\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.048709 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.049823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.050831 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.051777 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.052715 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.053645 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.054529 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.055444 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.056401 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.057359 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.058250 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.059118 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.060069 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.061006 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.062031 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.062949 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.064062 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.065333 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.066528 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.067678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.068804 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.069702 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.070614 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.071702 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.073702 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.075094 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.076227 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.077562 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.078702 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.079649 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.080546 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.081482 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.082423 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.083335 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.084213 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.085140 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.086357 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.087435 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.088371 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.089272 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.090287 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.091243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.092115 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.092963 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.093915 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.094858 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.095909 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.096896 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.097829 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.098725 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.099645 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.100685 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.101662 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.102544 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.103456 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.104379 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.105298 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.106199 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.107243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.108226 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.109152 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.110044 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.110920 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.111787 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.112817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.113788 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.114835 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.115805 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.117088 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.118494 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.119588 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.120732 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.121758 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.123383 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.124884 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.125833 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.148999 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.150814 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.152503 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.153976 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.155415 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.156841 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.158282 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.159718 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.161515 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.163404 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.164922 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.166565 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.168094 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.169552 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.171311 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.173035 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.174574 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.175955 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.177861 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.179828 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.181077 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.182070 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.183117 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.184056 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.184985 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.186079 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.187056 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.187863 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.188769 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.189875 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.190837 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.191704 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.192646 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.193678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.194639 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.195475 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.196392 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.197252 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.198129 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.199018 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.199863 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.200759 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.201772 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.202678 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.203723 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.204732 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.205583 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.206383 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.207249 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.208116 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.208956 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.209817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.210661 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.211580 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.212399 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.213187 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.214051 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.214870 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.215857 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.216754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.217607 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.218578 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.219473 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.220256 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.221041 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.221853 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.222743 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.223586 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.224551 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.225417 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.226279 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.227181 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.228269 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.229248 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.230251 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.231286 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.232672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.233672 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.234542 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.235411 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.236349 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.237243 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.238108 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.239014 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.239927 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.240856 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.241779 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.242641 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.243495 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.244414 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.245324 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.246257 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.247999 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.250081 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.251035 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.252063 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.252962 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.253817 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.254679 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.255585 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.256466 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.257491 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.258499 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.259399 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.260228 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.261126 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.262006 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.262970 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.263913 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.264774 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.265644 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.266555 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.267443 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.268521 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.269675 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.270602 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.271403 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.272202 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.273136 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.274168 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.275086 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.275980 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.276834 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.277773 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.278674 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.279711 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.280880 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.281826 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.282722 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.283800 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.284770 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.285645 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.286754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.287823 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.288765 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.289664 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.290648 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.291754 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.292666 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.293511 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.294410 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.295342 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.296284 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.297291 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.298279 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.299193 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.300208 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.301201 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.302139 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.303049 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.303959 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.304857 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.305740 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.306602 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.307555 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.308632 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.309601 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.310507 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.311599 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.312635 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.313539 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.314446 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.315349 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.316610 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.317749 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.318633 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.319496 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.320399 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.321253 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.322275 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.323205 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.324078 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.324978 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.325898 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.326719 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.327723 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.328698 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.329589 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.330492 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.331366 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.332283 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.333200 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.334109 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.335024 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.336049 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.337037 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.338152 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.339200 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.340104 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.340947 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.341771 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.342623 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.343545 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.344535 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.345449 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:using sentence piece tokenzier.\n",
            "I0109 00:41:44.346510 139651685500800 tokenization.py:237] using sentence piece tokenzier.\n",
            "INFO:tensorflow:***** Running evaluation *****\n",
            "I0109 00:41:44.830431 139651685500800 run_classifier.py:350] ***** Running evaluation *****\n",
            "INFO:tensorflow:  Num examples = 280 (277 actual, 3 padding)\n",
            "I0109 00:41:44.830691 139651685500800 run_classifier.py:353]   Num examples = 280 (277 actual, 3 padding)\n",
            "INFO:tensorflow:  Batch size = 8\n",
            "I0109 00:41:44.830811 139651685500800 run_classifier.py:354]   Batch size = 8\n",
            "INFO:tensorflow:Best trial info: Step: -1, Best Value Step: -1, Best Value: -1\n",
            "I0109 00:41:45.084237 139651685500800 run_classifier.py:391] Best trial info: Step: -1, Best Value Step: -1, Best Value: -1\n",
            "INFO:tensorflow:Add gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0 to eval list.\n",
            "I0109 00:41:45.215345 139651685500800 run_classifier.py:433] Add gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0 to eval list.\n",
            "INFO:tensorflow:Add gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60 to eval list.\n",
            "I0109 00:41:45.215595 139651685500800 run_classifier.py:433] Add gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60 to eval list.\n",
            "INFO:tensorflow:found 2 files.\n",
            "I0109 00:41:45.215664 139651685500800 run_classifier.py:435] found 2 files.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:41:45.221398 139651685500800 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b43bfdd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:41:45.239048 139651685500800 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b43bfdd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:41:45.311641 139651685500800 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (1, 512)\n",
            "I0109 00:41:45.311867 139651685500800 classifier_utils.py:826]   name = input_ids, shape = (1, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (1, 512)\n",
            "I0109 00:41:45.311946 139651685500800 classifier_utils.py:826]   name = input_mask, shape = (1, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (1,)\n",
            "I0109 00:41:45.312008 139651685500800 classifier_utils.py:826]   name = is_real_example, shape = (1,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (1,)\n",
            "I0109 00:41:45.312066 139651685500800 classifier_utils.py:826]   name = label_ids, shape = (1,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (1, 512)\n",
            "I0109 00:41:45.312123 139651685500800 classifier_utils.py:826]   name = segment_ids, shape = (1, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:41:45.312948 139651685500800 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:41:49.291409 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.291990 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.292305 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.292613 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.296068 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.302191 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.308396 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.311234 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.313074 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.320914 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.322058 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.332699 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.334381 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.343289 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.345022 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.353945 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.355665 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.374079 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.376664 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.382772 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.383970 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.390279 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.391752 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.401633 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.402834 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.408104 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:49.409225 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.451047 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.452272 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.459061 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.460146 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.463924 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.465010 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:41:50.469055 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:41:50.557121 139651685500800 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:41:50.706515 139651685500800 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:41:50.706738 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:41:50.706853 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:41:50.706933 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:41:50.707007 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:41:50.707076 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:41:50.707142 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:41:50.707212 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:41:50.707278 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:41:50.707410 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:41:50.707490 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:41:50.707561 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:41:50.707625 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:41:50.707694 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:41:50.707758 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:41:50.707826 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:41:50.707890 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:41:50.707953 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:41:50.708016 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:41:50.708083 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:41:50.708147 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:41:50.708214 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:41:50.708276 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:41:50.708384 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:41:50.708456 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:41:50.708524 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:41:50.708589 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:41:50.708657 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:41:50.708720 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:41:50.708783 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:41:50.708845 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:41:50.708908 139651685500800 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:41:50.708976 139651685500800 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0109 00:41:50.722507 139651685500800 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:41:51.821006 139651685500800 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-01-09T00:41:51Z\n",
            "I0109 00:41:51.837366 139651685500800 evaluation.py:255] Starting evaluation at 2022-01-09T00:41:51Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:41:51.837648 139651685500800 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:41:51.964937 139651685500800 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0\n",
            "I0109 00:41:52.095384 139651685500800 saver.py:1284] Restoring parameters from gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:41:56.850303 139651685500800 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:41:57.208045 139651685500800 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:41:57.954606 139651685500800 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 22 seconds\n",
            "I0109 00:42:20.602852 139651685500800 tpu_estimator.py:576] Initialized TPU in 22 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:42:20.603584 139649534260992 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:42:20.603955 139649525868288 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:42:20.990617 139651685500800 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (35) batch(es) of data to infeed.\n",
            "I0109 00:42:21.346749 139651685500800 tpu_estimator.py:600] Enqueue next (35) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0109 00:42:21.347071 139651685500800 tpu_estimator.py:604] Dequeue next (35) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:42:27.122303 139649525868288 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Evaluation [35/35]\n",
            "I0109 00:42:27.710891 139651685500800 evaluation.py:167] Evaluation [35/35]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:42:27.711174 139651685500800 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:42:27.711248 139651685500800 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:42:27.711392 139649534260992 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:42:27.711464 139649534260992 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:42:27.711643 139651685500800 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:42:27.711730 139651685500800 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:42:27.711781 139651685500800 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:42:27.711911 139649525868288 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:42:27.712035 139649525868288 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:42:27.712162 139651685500800 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:42:27.712304 139651685500800 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2022-01-09-00:42:28\n",
            "I0109 00:42:28.574833 139651685500800 evaluation.py:275] Finished evaluation at 2022-01-09-00:42:28\n",
            "INFO:tensorflow:Saving dict for global step 0: eval_accuracy = 0.5306859, eval_loss = 0.7007507, global_step = 0, loss = 0.6778714\n",
            "I0109 00:42:28.575138 139651685500800 estimator.py:2049] Saving dict for global step 0: eval_accuracy = 0.5306859, eval_loss = 0.7007507, global_step = 0, loss = 0.6778714\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0\n",
            "I0109 00:42:33.129487 139651685500800 estimator.py:2109] Saving 'checkpoint_path' summary for global step 0: gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-0\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "I0109 00:42:34.097489 139651685500800 error_handling.py:101] evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "I0109 00:42:34.097766 139651685500800 run_classifier.py:453] ***** Eval results *****\n",
            "INFO:tensorflow:  eval_accuracy = 0.5306859\n",
            "I0109 00:42:34.097879 139651685500800 run_classifier.py:455]   eval_accuracy = 0.5306859\n",
            "INFO:tensorflow:  eval_loss = 0.7007507\n",
            "I0109 00:42:34.098180 139651685500800 run_classifier.py:455]   eval_loss = 0.7007507\n",
            "INFO:tensorflow:  global_step = 0\n",
            "I0109 00:42:34.098363 139651685500800 run_classifier.py:455]   global_step = 0\n",
            "INFO:tensorflow:  loss = 0.6778714\n",
            "I0109 00:42:34.098480 139651685500800 run_classifier.py:455]   loss = 0.6778714\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0109 00:42:35.014640 139651685500800 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b43bf7a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0109 00:42:35.030641 139651685500800 ag_logging.py:146] Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f02b43bf7a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0109 00:42:35.104466 139651685500800 classifier_utils.py:824] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (1, 512)\n",
            "I0109 00:42:35.104713 139651685500800 classifier_utils.py:826]   name = input_ids, shape = (1, 512)\n",
            "INFO:tensorflow:  name = input_mask, shape = (1, 512)\n",
            "I0109 00:42:35.104792 139651685500800 classifier_utils.py:826]   name = input_mask, shape = (1, 512)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (1,)\n",
            "I0109 00:42:35.104857 139651685500800 classifier_utils.py:826]   name = is_real_example, shape = (1,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (1,)\n",
            "I0109 00:42:35.104918 139651685500800 classifier_utils.py:826]   name = label_ids, shape = (1,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (1, 512)\n",
            "I0109 00:42:35.104979 139651685500800 classifier_utils.py:826]   name = segment_ids, shape = (1, 512)\n",
            "INFO:tensorflow:creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "I0109 00:42:35.105840 139651685500800 fine_tuning_utils.py:65] creating model from hub_module: https://tfhub.dev/google/albert_base/3\n",
            "E0109 00:42:39.378252 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.378880 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.379237 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.379567 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.382948 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.389191 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.395172 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.398781 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.399986 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.408051 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.409171 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/embedding_hidden_mapping_in/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.419632 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.421361 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.430169 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.431858 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.441755 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.443470 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.461409 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.463326 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.469424 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.471210 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.482751 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.485429 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.498165 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.499368 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.504740 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:39.505872 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.326510 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.327722 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.334705 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.335825 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.340121 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.341172 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0109 00:42:40.345740 139651685500800 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0109 00:42:40.436726 139651685500800 saver.py:1503] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0109 00:42:40.583088 139651685500800 classifier_utils.py:861] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0109 00:42:40.583343 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0109 00:42:40.583461 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0109 00:42:40.583542 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:42:40.583616 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:42:40.583684 139651685500800 classifier_utils.py:867]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0109 00:42:40.583749 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0109 00:42:40.583818 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0109 00:42:40.583883 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0109 00:42:40.583951 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0109 00:42:40.584015 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0109 00:42:40.584082 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0109 00:42:40.584146 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0109 00:42:40.584213 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:42:40.584318 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:42:40.584396 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0109 00:42:40.584462 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0109 00:42:40.584525 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0109 00:42:40.584587 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0109 00:42:40.584653 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0109 00:42:40.584716 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0109 00:42:40.584783 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0109 00:42:40.584847 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0109 00:42:40.584910 139651685500800 classifier_utils.py:867]   name = module/bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0109 00:42:40.584972 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0109 00:42:40.585039 139651685500800 classifier_utils.py:867]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0109 00:42:40.585102 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0109 00:42:40.585169 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0109 00:42:40.585232 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0109 00:42:40.585295 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0109 00:42:40.585373 139651685500800 classifier_utils.py:867]   name = module/cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
            "I0109 00:42:40.585438 139651685500800 classifier_utils.py:867]   name = output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
            "I0109 00:42:40.585505 139651685500800 classifier_utils.py:867]   name = output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0109 00:42:41.707494 139651685500800 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-01-09T00:42:41Z\n",
            "I0109 00:42:41.724303 139651685500800 evaluation.py:255] Starting evaluation at 2022-01-09T00:42:41Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "I0109 00:42:41.724574 139651685500800 tpu_estimator.py:506] TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0109 00:42:41.843381 139651685500800 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60\n",
            "I0109 00:42:41.974623 139651685500800 saver.py:1284] Restoring parameters from gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0109 00:42:46.940785 139651685500800 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0109 00:42:47.276157 139651685500800 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "I0109 00:42:47.988588 139651685500800 tpu_estimator.py:567] Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 22 seconds\n",
            "I0109 00:43:10.463978 139651685500800 tpu_estimator.py:576] Initialized TPU in 22 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "I0109 00:43:10.464857 139649525868288 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "I0109 00:43:10.465267 139649517475584 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "I0109 00:43:10.841210 139651685500800 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (35) batch(es) of data to infeed.\n",
            "I0109 00:43:11.206503 139651685500800 tpu_estimator.py:600] Enqueue next (35) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0109 00:43:11.206855 139651685500800 tpu_estimator.py:604] Dequeue next (35) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "I0109 00:43:17.082479 139649517475584 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Evaluation [35/35]\n",
            "I0109 00:43:17.671831 139651685500800 evaluation.py:167] Evaluation [35/35]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "I0109 00:43:17.672183 139651685500800 tpu_estimator.py:608] Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "I0109 00:43:17.672328 139651685500800 tpu_estimator.py:434] Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "I0109 00:43:17.672518 139649525868288 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "I0109 00:43:17.672610 139649525868288 tpu_estimator.py:537] Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "I0109 00:43:17.672741 139651685500800 error_handling.py:101] infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "I0109 00:43:17.672863 139651685500800 tpu_estimator.py:612] Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "I0109 00:43:17.672944 139651685500800 tpu_estimator.py:434] Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "I0109 00:43:17.673079 139649517475584 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "I0109 00:43:17.673155 139649517475584 tpu_estimator.py:551] Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "I0109 00:43:17.673276 139651685500800 error_handling.py:101] outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "I0109 00:43:17.673422 139651685500800 tpu_estimator.py:616] Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2022-01-09-00:43:18\n",
            "I0109 00:43:18.518266 139651685500800 evaluation.py:275] Finished evaluation at 2022-01-09-00:43:18\n",
            "INFO:tensorflow:Saving dict for global step 60: eval_accuracy = 0.47292417, eval_loss = 108.49415, global_step = 60, loss = 99.98019\n",
            "I0109 00:43:18.518663 139651685500800 estimator.py:2049] Saving dict for global step 60: eval_accuracy = 0.47292417, eval_loss = 108.49415, global_step = 60, loss = 99.98019\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 60: gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60\n",
            "I0109 00:43:19.252037 139651685500800 estimator.py:2109] Saving 'checkpoint_path' summary for global step 60: gs://luanps/albert-tfhub/models/RTE/0f541297/model.ckpt-60\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "I0109 00:43:19.966571 139651685500800 error_handling.py:101] evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "I0109 00:43:19.966851 139651685500800 run_classifier.py:453] ***** Eval results *****\n",
            "INFO:tensorflow:  eval_accuracy = 0.47292417\n",
            "I0109 00:43:19.966933 139651685500800 run_classifier.py:455]   eval_accuracy = 0.47292417\n",
            "INFO:tensorflow:  eval_loss = 108.49415\n",
            "I0109 00:43:19.967036 139651685500800 run_classifier.py:455]   eval_loss = 108.49415\n",
            "INFO:tensorflow:  global_step = 60\n",
            "I0109 00:43:19.967115 139651685500800 run_classifier.py:455]   global_step = 60\n",
            "INFO:tensorflow:  loss = 99.98019\n",
            "I0109 00:43:19.967180 139651685500800 run_classifier.py:455]   loss = 99.98019\n",
            "INFO:tensorflow:saving model.ckpt-0.meta to model.ckpt-best.meta\n",
            "I0109 00:43:21.308808 139651685500800 run_classifier.py:473] saving model.ckpt-0.meta to model.ckpt-best.meta\n",
            "INFO:tensorflow:saving model.ckpt-0.data-00000-of-00001 to model.ckpt-best.data-00000-of-00001\n",
            "I0109 00:43:22.102157 139651685500800 run_classifier.py:473] saving model.ckpt-0.data-00000-of-00001 to model.ckpt-best.data-00000-of-00001\n",
            "INFO:tensorflow:saving model.ckpt-0.index to model.ckpt-best.index\n",
            "I0109 00:43:22.903283 139651685500800 run_classifier.py:473] saving model.ckpt-0.index to model.ckpt-best.index\n",
            "cp gs://luanps/albert-tfhub/models/RTE/0f541297/eval_results.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-09 00:43:24,428]\u001b[0m Trial 1 finished with value: 0.7545126 and parameters: {'warmup_steps': 5, 'train_steps': 60, 'learning_rate': 20.461353439247414, 'batch_size': 112}. Best is trial 0 with value: 0.7545126.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.trials_dataframe()"
      ],
      "metadata": {
        "id": "SX9kEMe6Oolq",
        "outputId": "8c3b11ce-f6e7-418a-ca02-d4a01ed3fa4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-91def90f-a0b0-4b86-89ad-8b53deeef47d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number</th>\n",
              "      <th>value</th>\n",
              "      <th>datetime_start</th>\n",
              "      <th>datetime_complete</th>\n",
              "      <th>duration</th>\n",
              "      <th>params_batch_size</th>\n",
              "      <th>params_learning_rate</th>\n",
              "      <th>params_train_steps</th>\n",
              "      <th>params_warmup_steps</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.754513</td>\n",
              "      <td>2022-01-09 00:35:02.616274</td>\n",
              "      <td>2022-01-09 00:39:06.571024</td>\n",
              "      <td>0 days 00:04:03.954750</td>\n",
              "      <td>80</td>\n",
              "      <td>0.038340</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.754513</td>\n",
              "      <td>2022-01-09 00:39:06.579229</td>\n",
              "      <td>2022-01-09 00:43:24.427390</td>\n",
              "      <td>0 days 00:04:17.848161</td>\n",
              "      <td>112</td>\n",
              "      <td>20.461353</td>\n",
              "      <td>60</td>\n",
              "      <td>5</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91def90f-a0b0-4b86-89ad-8b53deeef47d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91def90f-a0b0-4b86-89ad-8b53deeef47d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91def90f-a0b0-4b86-89ad-8b53deeef47d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   number     value  ... params_warmup_steps     state\n",
              "0       0  0.754513  ...                  10  COMPLETE\n",
              "1       1  0.754513  ...                   5  COMPLETE\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pack Optuna results and save to Bucket\n",
        "import joblib\n",
        "\n",
        "study_file = f'{TASK}_study.pkl'\n",
        "joblib.dump(study, study_file)\n",
        "!gsutil cp $study_file $OUTPUT_DIR"
      ],
      "metadata": {
        "id": "jjsSV2pJMelC",
        "outputId": "b195aebb-a62e-4d03-ecfe-000661a4827a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://RTE_study.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  7.8 KiB/  7.8 KiB]                                                \n",
            "Operation completed over 1 objects/7.8 KiB.                                      \n"
          ]
        }
      ]
    }
  ]
}